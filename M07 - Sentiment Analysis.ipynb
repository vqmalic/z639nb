{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Broadly speaking, Sentiment Analysis is the task of identifying and extracting subjective information from a source material - in our case, our source material will exclusively be text. \n",
    "\n",
    "Defined as such, Sentiment Analysis is a *huge* subfield of text mining. It's hard to overstate how many routes you may take when the overarching goal is the identification of subjectivity. In today's lecture, we're only going to learn how to implement the most basic sentiment analysis approaches. If your final project merits a more sophisticated approach, then be sure to do your own reading and to consult with me when necessary. There's a reason why the final project is the only major graded component of the class.\n",
    "\n",
    "All of you interested in Sentiment Analysis must be made aware (if you don't already know) that *the* canonical reference text, published in 2008 but still used today, is [the monograph *Opinion Mining and Sentiment Analysis* by Bo Pang and Lillian Lee](http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf). One reason why it has retained its salience (and gleaned over 5000 citations) is that it is primarily a treatise on the the *problem* of sentiment analysis, rather than a demonstration of a particular machine learning technique. As such, it remains relevant today. Much of today's talk and demos will be based, directly or indirectly, on Pang's article. That being said, **if you're considering working with sentiment for your final project, you should probably read the Pang monograph**. \n",
    "\n",
    "## Polarity Classification\n",
    "\n",
    "This is probably the first thing you think of when you hear the phrase \"sentiment analysis\", and this is the topic we'll be covering in today's lecture. Polarity classification is sentiment analysis in its most simple form. Is a sentence positive or negative? Assign a value of 1 if positive, or 0 if negative. Thumbs up or thumb down, good or bad, happy or sad. You have two mutually exclusively and oppositional *polarities* and you're assuming that a given text belongs to one or the other. \n",
    "\n",
    "You are an excellent polarity classifier. Assign the label \"positive\" or \"negative\" to the following sentences. \n",
    "\n",
    "* This movie was terrific! \n",
    "* This movie totally sucked!\n",
    "\n",
    "Trivially easy. But how do we train a computer to do the same thing? Morever, how do we train it to identify patterns in text such that it can continue to classify new, unseen, incoming texts as positive or negative?\n",
    "\n",
    "Let's think for a moment how you were able to quickly classify the above two example sentences. They're both sentences with 4 words, but the dead giveaway were the last words in each sentences. Those words were *adjectives* and they had a *clear polarity*. Terrific is a positive word, sucked is a negative word. \n",
    "\n",
    "Having observed this, we have a simple - but potentially effective - way to classify texts. Look in the texts, and see if they have positive or negative adjectives. Let's try this on some data. We'll analyze the most recent 2500 Tweets from both the Clinton and Trump accounts. I know you're probably getting tired of these accounts, but as I tried to come up with a different dataset, I realized that you really can't top the election when it comes to finding highly emotional and polarized tweets. \n",
    "\n",
    "I've already shown how to pull Tweets using the Tweepy API, so I've already downloaded the Tweets and saved them into a csv file using Pandas. Here, I'm loading it back into the Python environment. The csv file will be available on canvas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"sentimentanalysis.csv\", index_col=0, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>fav</th>\n",
       "      <th>rt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Remember, don't believe \"sources said\" by the ...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>8572</td>\n",
       "      <td>3616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Did Crooked Hillary help disgusting (check out...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>12930</td>\n",
       "      <td>5628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Using Alicia M in the debate as a paragon of v...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>12738</td>\n",
       "      <td>5209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Wow, Crooked Hillary was duped and used by my ...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>13210</td>\n",
       "      <td>5205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Anytime you see a story about me or my campaig...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>21316</td>\n",
       "      <td>9147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              user                                               text  \\\n",
       "0  realDonaldTrump  Remember, don't believe \"sources said\" by the ...   \n",
       "1  realDonaldTrump  Did Crooked Hillary help disgusting (check out...   \n",
       "2  realDonaldTrump  Using Alicia M in the debate as a paragon of v...   \n",
       "3  realDonaldTrump  Wow, Crooked Hillary was duped and used by my ...   \n",
       "4  realDonaldTrump  Anytime you see a story about me or my campaig...   \n",
       "\n",
       "                source    fav    rt  \n",
       "0  Twitter for Android   8572  3616  \n",
       "1  Twitter for Android  12930  5628  \n",
       "2  Twitter for Android  12738  5209  \n",
       "3  Twitter for Android  13210  5205  \n",
       "4  Twitter for Android  21316  9147  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's go ahead and iterate through all the tweet texts and mark each tweet that contains the word \"good\" as 1 and each tweet that contains the word \"bad\" as -1. We'll leave a neutral category, 0, for the likely possibility that a tweet does not contain either word. This will be our first, brute-force sentiment analysis. \n",
    "\n",
    "Note that in its current format, the Tweet text is a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Remember, don\\'t believe \"sources said\" by the VERY dishonest media. If they don\\'t name the sources, the sources don\\'t exist.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we iterate through the tweets, we need some solid method for splitting each tweet into a list of words; a process known as tokenization. \n",
    "\n",
    "We'll use the Natural Language Toolkit, or ``nltk``. It has a word tokenizer. \n",
    "\n",
    "If this is your first time using NLTK, you may have to use the command line to install some peripherals. This is covered in the lecture video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sentence', 'is', 'a', 'string', ',', 'but', 'after', 'it', \"'s\", 'passed', 'through', 'word_tokenize', ',', 'it', 'becomes', 'a', 'list', 'of', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "print(word_tokenize(\"This sentence is a string, but after it's passed through word_tokenize, it becomes a list of words.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we'll iterate through all the sentences, see if a positive or negative word is in it, and assign it a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_labels = []\n",
    "\n",
    "for text in df['text']:\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    if \"good\" in tokenized_text:\n",
    "        sentiment_labels.append(1) # This is a positive tweet; append 1\n",
    "    elif \"bad\" in tokenized_text:\n",
    "        sentiment_labels.append(-1) # This is a negative tweet; append -1\n",
    "    else:\n",
    "        sentiment_labels.append(0) # This tweet has no identifiable sentiment (by our current, simple model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the newly-created sentiment labels to the data frame as a new column\n",
    "df['sentiment'] = sentiment_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate our results. First, how many good, bad, and neutral tweets did we find for Clinton and Trump?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user             sentiment\n",
       "HillaryClinton   -1              7\n",
       "                  0           2446\n",
       "                  1             47\n",
       "realDonaldTrump  -1             64\n",
       "                  0           2393\n",
       "                  1             43\n",
       "Name: user, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"user\", \"sentiment\"])[\"user\"].agg(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 negative and 47 positive Tweets for Clinton, 2446 unclassified. 64 negative and 43 positive for Trump, 2393 unclassified. For obvious reasons, we didn't capture very much sentiment - we're only using one positive and one negative word as our distinguisher so our model is low powered. \n",
    "\n",
    "Now, let's take a look at what positive and negative Tweets we found for the candidates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinton, Negative\n",
      "\"Donald Trump doesn't see people like me, he only sees disability. I honestly feel bad for someone with so much hat",
      " https://t.co/NTeF9u8JpN\n",
      "**************************************************\n",
      "RT @RevDrBarber: Not only is @realDonaldTrump wrong on birtherism, his words and policies are bad for Americans alive now and our children",
      "\n",
      "**************************************************\n",
      "3. While refusing to release your tax returns, how will you confirm that you do not have dangerous financial ties to bad actors abroad?\n",
      "**************************************************\n",
      "A wall that Mexico will pay for: a bad idea from an even worse negotiator. https://t.co/RZfOKe45RO\n",
      "**************************************************\n",
      "Donald Trump's economic plan is a bad deal for working familiesand a big tax cut for Donald Trump.\n",
      "https://t.co/D0oOe6bwza\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "c_neg = df[(df['user']==\"HillaryClinton\") & (df['sentiment']==-1)]\n",
    "print(\"Clinton, Negative\")\n",
    "for t in c_neg['text'][:5]:\n",
    "    print(t)\n",
    "    print(\"*\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping in mind that we have an extremely simple and blunt model, it looks like the tweets from Clinton that we marked as negative are, in fact, negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump, Negative\n",
      ".@HillaryClinton channels John Kerry on trade: she was for bad trade deals before she was against them. #TPP #Debates2016\n",
      "**************************************************\n",
      "Crooked Hillary's bad judgement forced her to announce that she would go to Charlotte on Saturday to grandstand. Dem pols said no way, dumb!\n",
      "**************************************************\n",
      "President Obama &amp; Putin fail to reach deal on Syria - so what else is new? Obama is not a natural deal maker. Only makes bad deals!\n",
      "**************************************************\n",
      "Now that African-Americans are seeing what a bad job Hillary type policy and management has done to the inner-cities, they want TRUMP!\n",
      "**************************************************\n",
      "Crooked Hillary's brainpower is highly overrated.Probably why her decision making is so bad or, as stated by Bernie S, she has BAD JUDGEMENT\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "t_neg = df[(df['user']==\"realDonaldTrump\") & (df['sentiment']==-1)]\n",
    "print(\"Trump, Negative\")\n",
    "for t in t_neg['text'][:5]:\n",
    "    print(t)\n",
    "    print(\"*\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've correctly labeled some negative Trump tweets as well.\n",
    "\n",
    "Now on to the positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinton, Positive\n",
      "When you have a really, really good night. #SheWon https://t.co/FVeGqhYxRZ\n",
      "**************************************************\n",
      "\"Anyone who complains about microphone problems is not having a good night.\" Hillary #SheWon\n",
      "**************************************************\n",
      "\"Maybe he didn't do a good job.\" Donald Trump\n",
      "\n",
      "Looks like you loved it at the time. #DebateNight",
      " https://t.co/LelC6Tb3nj\n",
      "**************************************************\n",
      "Donald Trump is really good at spending other people's moneyon:\n",
      "\n",
      "Legal settlements.\n",
      "Payoffs.\n",
      "Portraits of himself. https://t.co/C5bXw5brI9\n",
      "**************************************************\n",
      "\"You want to give me a good send-off? Go vote. Barack Obama: https://t.co/tTgeqxNqYm https://t.co/Jqf2jmx3D0\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "c_pos = df[(df['user']==\"HillaryClinton\") & (df['sentiment']==1)]\n",
    "print(\"Clinton, Positive\")\n",
    "for t in c_pos['text'][:5]:\n",
    "    print(t)\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, some of the difficulties of sentiment analysis appears. \n",
    "\n",
    "How would you evaluate the sentiment of the 3rd tweet? \n",
    "\n",
    "\"Maybe he didn't do a good job.\" has the word \"good\" in it. But it's a *negated* sentence. Someone *didn't* do a good job. \n",
    "\n",
    "Furthermore, this is a *quote* inside of the Tweet. Even a human would have a hard time labeling this one as positive or negative. \n",
    "\n",
    "Look at the 4th tweet: Clinton says Donald Trump is good at something - but she's being sarcastic.\n",
    "\n",
    "Let's look at Trump's \"positive\" tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump, Positive\n",
      "Heading to Colorado for a big rally. Massive crowd, great people! Will be there soon - the polls are looking good.\n",
      "**************************************************\n",
      "Poll numbers are starting to look very good. Leading in Florida @CNN Arizona and big jump in Utah. All numbers rising, national way up. Wow!\n",
      "**************************************************\n",
      "\"@Brainykid2010: @shl @realDonaldTrump The ad was actually very good!\"\n",
      "**************************************************\n",
      "You have no idea what my strategy on ISIS is, and neither does ISIS (a good thing). Please get your facts straight - thanks. @megynkelly\n",
      "**************************************************\n",
      "Funny, if you listen to @FoxNews, the Democrats did not have a good day. If you listen to the other two, they are fawning. What a difference\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "t_pos = df[(df['user']==\"realDonaldTrump\") & (df['sentiment']==1)]\n",
    "print(\"Trump, Positive\")\n",
    "for t in t_pos['text'][:5]:\n",
    "    print(t)\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, some complications arise. We once again see a negation in the last tweet: \"Democrats did not have a good day.\" The fourth Tweet is very ambiguous. Is it good or bad? \n",
    "\n",
    "## Knowledge-Based Sentiment Analysis\n",
    "\n",
    "The form of sentiment analysis we just conducted is known as a **knowledge-based** analysis. Why? Because we referred to a human-made, pre-existing knowledge base to make our sentiment classification decisions. Namely, a two-item knowledge base: \"good\" indicates a positive tweet, \"bad\" indicates a negative tweet. \n",
    "\n",
    "We need to improve our classification scheme to account for some of the subtleties of human language. When it comes to using a knowledge-based technique, one way to improve things is to have a better knowledge base. Right now we're using only two words. How can we make this situation better?\n",
    "\n",
    "Well, one simple and obvious idea is to have *more indicator words*. We can make a *pool* of words that indicate positivity, and another pool of words indicating negativity.\n",
    "\n",
    "Another thing we can do is introduce a sort of score, rather than working in binary. Obviously, the word \"awful\" is stronger than the word \"bad.\" Maybe we can assign \"awful\" a value of -2 and bad a value of -1. \n",
    "\n",
    "In this manner, with a dictionary of positive words *and* their weights, and negative words *and* their weights, we can assign a *total score* to each Tweet by counting all of the positive negative words, finding their weights, and simply adding them together. \n",
    "\n",
    "The *biggest problem* with knowledge bases is that they must be manually created. You have to manually create a set of positive and negative words, and assign weights to them. This is tedious, and time consuming. \n",
    "\n",
    "Fortunately for us: several researchers have taken up this tedious work and have made lists of positive and negative words *and* assigned them weights. There are several to chose from, but in today's lecture we'll talk about using the [AFINN dataset by Finn Årup Nielsen](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010).\n",
    "\n",
    "Here are some random words from the AFINN dataset:\n",
    "\n",
    "* abandon -2\n",
    "* affection 3\n",
    "* anxious -2\n",
    "* applauded 2\n",
    "* best 3\n",
    "\n",
    "And so on. There are 2477 words and phrases annotated as such. To obtain a polarity score for a given text, simply identify all words in the text that are in AFINN, and add up their corresponding weight. If it's above 0, it's mostly positive, if it's below, it's mostly negative. Still a simple approach, similar to what we did above, but now we can use 2477 words with different weights rather than 2 words with 2 weights. How much improvement do we gain with this wider knowledge base?\n",
    "\n",
    "Luckly for us, there is an existing Python module that will execute the above steps for us. It's called ``afinn``. It doesn't come with Anaconda, so we'll have to install it. In your command line or terminal environment, type:\n",
    "\n",
    "```\n",
    "pip install afinn\n",
    "\n",
    "```\n",
    "\n",
    "To get it installed. Once you've done that, you have to import the factory method, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you have to use the factory method to make an instance of an afinn object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "afinn = Afinn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the ``afinn`` object, we simply call its method score and pass it a string of text. We don't even have to tokenize the text using NLTK, ``afinn`` do it for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afinn.score(\"What a wonderful day! The sun is shining and the birds are singing. I feel great.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afinn.score(\"What a dreary and depressing day. It's raining and I don't even own an umbrella.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an empty list called pscore (for \"polarity score\"). Then, let's iterate through all the Tweet texts and append its corresponding AFINN score to the pscore list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.0, -1.0, -5.0, -6.0, -1.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pscore = []\n",
    "\n",
    "for text in df['text']:\n",
    "    pscore.append(afinn.score(text))\n",
    "    \n",
    "pscore[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add the pscores as a column to our Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>fav</th>\n",
       "      <th>rt</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>pscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Remember, don't believe \"sources said\" by the ...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>8572</td>\n",
       "      <td>3616</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Did Crooked Hillary help disgusting (check out...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>12930</td>\n",
       "      <td>5628</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Using Alicia M in the debate as a paragon of v...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>12738</td>\n",
       "      <td>5209</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Wow, Crooked Hillary was duped and used by my ...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>13210</td>\n",
       "      <td>5205</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Anytime you see a story about me or my campaig...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>21316</td>\n",
       "      <td>9147</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              user                                               text  \\\n",
       "0  realDonaldTrump  Remember, don't believe \"sources said\" by the ...   \n",
       "1  realDonaldTrump  Did Crooked Hillary help disgusting (check out...   \n",
       "2  realDonaldTrump  Using Alicia M in the debate as a paragon of v...   \n",
       "3  realDonaldTrump  Wow, Crooked Hillary was duped and used by my ...   \n",
       "4  realDonaldTrump  Anytime you see a story about me or my campaig...   \n",
       "\n",
       "                source    fav    rt  sentiment  pscore  \n",
       "0  Twitter for Android   8572  3616          0    -2.0  \n",
       "1  Twitter for Android  12930  5628          0    -1.0  \n",
       "2  Twitter for Android  12738  5209          0    -5.0  \n",
       "3  Twitter for Android  13210  5205          0    -6.0  \n",
       "4  Twitter for Android  21316  9147          0    -1.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pscore'] = pscore\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have essentially \"created\" a new feature that didn't exist before - the polarity score.\n",
    "\n",
    "Let's investigate how well this method actually works. First, I'm going to split the data frame into two subsets, one containing a single candidate's tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton = df[df['user'] == \"HillaryClinton\"]\n",
    "trump = df[df['user'] == \"realDonaldTrump\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the Clinton and Trump tweets with the highest negative polarity. I'm going to use a method called ``sort_values``. It takes a named argument called ``by`` which indicates which column you want to sort by. Sorting, in this case, means going from lowest to highest, so once we've done this the top five rows will be the **most negative** (have the lowest values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUMP, LOWEST POLARITY\n",
      "--------------------------------------------------\n",
      "('Crooked Hillary Clinton is \"guilty as hell\" but the system is totally rigged and corrupt! Where are the 33,000 missing e-mails?', -13.0)\n",
      "**************************************************\n",
      "('My heart &amp; prayers go out to all of the victims of the terrible #Brussels tragedy. This madness must be stopped, and I will stop it.', -13.0)\n",
      "**************************************************\n",
      "('Crooked Hillary Clinton is a fraud who has put the public and country at risk by her illegal and very stupid use of e-mails. Many missing!', -13.0)\n",
      "**************************************************\n",
      "('My lawyers want to sue the failing @nytimes so badly for irresponsible intent. I said no (for now), but they are watching. Really disgusting', -12.0)\n",
      "**************************************************\n",
      "('Hillary Clinton is unfit to be president. She has bad judgement, poor leadership skills and a very bad and destructive track record. Change!', -12.0)\n",
      "**************************************************\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"TRUMP, LOWEST POLARITY\")\n",
    "print(\"-\"*50)\n",
    "trump = trump.sort_values(by='pscore')\n",
    "for index, row in trump.head().iterrows():\n",
    "    print((row['text'], row['pscore']))\n",
    "    print(\"*\"*50)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLINTON, LOWEST POLARITY\n",
      "--------------------------------------------------\n",
      "('It\\x92s wrong to tear each other down.\\nIt\\x92s wrong to let income inequality get worse.\\nIt\\x92s wrong to put a loose cannon in charge.', -12.0)\n",
      "**************************************************\n",
      "(\"Gun violence and hate aren't isolated\\x97homophobia in Orlando, racism in Charleston. We need to fight them together. https://t.co/HcBJzknBz7\", -12.0)\n",
      "**************************************************\n",
      "('There is something wrong with our country. There is too much violence...too much senseless killing, too many people dead who shouldn\\x92t be.', -11.0)\n",
      "**************************************************\n",
      "('We owe families of gun violence victims more than prayers.\\n\\nTell your senators to act on gun violence prevention: https://t.co/v0defjTptE', -11.0)\n",
      "**************************************************\n",
      "('RT @repjohnlewis: .@SpeakerRyan, we will not leave without acting for the victims &amp; families of reckless gun violence. #NoBillNoBreak https\\x85', -10.0)\n",
      "**************************************************\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"CLINTON, LOWEST POLARITY\")\n",
    "print(\"-\"*50)\n",
    "clinton = clinton.sort_values(by='pscore')\n",
    "for index, row in clinton.head().iterrows():\n",
    "    print((row['text'], row['pscore']))\n",
    "    print(\"*\"*50)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a very blunt and unsophisticated way to score polarity, this appears to have work quite well. The lowest-scoring tweets for both candidates do, in fact, seem to have very negative sentiment. \n",
    "\n",
    "How about the positive? Above, I used a method on the data frame called ``.head()`` to print out the first five rows of the sorted data frame. Incidentally, there is a method called ``.tail()`` that prints out the last five rows. Since the data frame is sorted, the last five rows will have the highest polarity scores, e.g. be the most positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUMP, HIGHEST POLARITY\n",
      "--------------------------------------------------\n",
      "('People (pundits) gave me no chance in South Carolina. Now it looks like a possible win. I would be happy with a one vote victory! (HOPE)', 15.0)\n",
      "**************************************************\n",
      "('As expected, the media is very much against me. Their dishonesty is amazing but, just like our big wins in the primaries, we will win!', 15.0)\n",
      "**************************************************\n",
      "('\"@PaulaDuvall2: We\\'re all enjoying you, as well, Mr. T.! You\\'ve inspired Hope and a Positive Spirit throughout America! God bless you!\" Nice', 15.0)\n",
      "**************************************************\n",
      "('Fun to watch the Democrats working so hard to win the great State of South Carolina when I just won the Republican version - amazing people!', 17.0)\n",
      "**************************************************\n",
      "('Great honor to be endorsed by popular &amp; successful @gov_gilmore of VA. A state that I very much want to win-THX Jim! https://t.co/x4Y1TAFHvn', 18.0)\n",
      "**************************************************\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"TRUMP, HIGHEST POLARITY\")\n",
    "print(\"-\"*50)\n",
    "trump = trump.sort_values(by='pscore')\n",
    "for index, row in trump.tail().iterrows():\n",
    "    print((row['text'], row['pscore']))\n",
    "    print(\"*\"*50)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLINTON, HIGHEST POLARITY\n",
      "--------------------------------------------------\n",
      "('\\x93I married my best friend. I was still in awe of\\x85how smart and strong and loving and caring she was.\\x94 \\x97@BillClinton on Hillary', 11.0)\n",
      "**************************************************\n",
      "(\"Our first-ever gold medal-winning women's gymnastics team: the Magnificent Seven. https://t.co/MVkhHBh7hI\", 12.0)\n",
      "**************************************************\n",
      "(\"Happy Fourth of July! Today let's celebrate the best of America\\x97our freedom, diversity, and the values we share. -H https://t.co/C6Mdt4iVg5\", 12.0)\n",
      "**************************************************\n",
      "('The people taking care of our children and our parents deserve a good wage, good benefits, and a secure retirement.', 12.0)\n",
      "**************************************************\n",
      "(\"RT @clairecmc: He's humble.He's incredibly smart.He's a joyful warrior,a kind &amp; thoughtful person.He'll be a wonderful VP! Congrats to my f\\x85\", 15.0)\n",
      "**************************************************\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"CLINTON, HIGHEST POLARITY\")\n",
    "print(\"-\"*50)\n",
    "clinton = clinton.sort_values(by='pscore')\n",
    "for index, row in clinton.tail().iterrows():\n",
    "    print((row['text'], row['pscore']))\n",
    "    print(\"*\"*50)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, not bad. The \"most positive\" tweets as identified by ``afinn`` do, in fact, seem quite positive. \n",
    "\n",
    "Now that we have this new feature, it might be a good idea to see how they're distributed. I'm using a boxplot visualization to show the ``pscore`` for the tweets; one boxplot per candidate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAgEgQ8AgSDwASAQBD4ABILAB4BA/D9WxcIRZVGqEAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2185d148208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.boxplot([clinton['pscore'], trump['pscore']], labels=[\"Clinton\", \"Trump\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in aggregate, both candidates have an average around 0, but that Trump's distribution has a wider spread. In other words, when Trump writes a positive tweet, it tends to be very positive, and when he rights a negative one, it tends to be more negative, while Clinton's tweets are more reserved when it comes to both positive and negative sentiment.\n",
    "\n",
    "To repeat, what we've done above are knowledge-based sentiment analysis; we rely on an external knowledge base to help us determine the sentiment of Tweets. The quality of our sentiment judgements, of course, is reliant on the quality of our knowledge base. Here, we used AFINN, but there are a lot of options out there. \n",
    "\n",
    "* AFINN (which we just used)\n",
    "* [General Inquirer](http://www.wjh.harvard.edu/~inquirer/)\n",
    "* [Liu Bing's Sentiment Lexicons](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html)\n",
    "* [Sentiwordnet](http://sentiwordnet.isti.cnr.it/), which is actually included in NLTK\n",
    "\n",
    "## Making your own sentiment vocabulary using seeds\n",
    "\n",
    "You might be working with Tweets from a very specialized domain; the example we're using right now is a good illustration. These are political tweets, so words that indicate \"positive\" and \"negative\" sentiment in the political realm may not be the same as words that are positive or negative *generally*. \n",
    "\n",
    "It's possible if you're working in a specific domain like politics or product reviews, someone out there has already made a sentiment lexicon calibrated to your specific domain. If that's the case, you're in luck. More like than not, however, you won't find a pre-made lexicon. \n",
    "\n",
    "You'll therefore have to *make* a lexicon. You *could* do this manually, but that would demand a huge amount of energy, time, and human resources. Probably not feasible for the duration of this course. There is, however, one way to make a sentiment lexicon manually. This process was first introduced by [Turney in a 2002 paper](http://www.aclweb.org/anthology/P02-1053.pdf) and involves starting with a small set of *seed words* that are positive or negative, and then *finding words that are strongly associated with those seed words*. Simply stated, if a word consistently appears with the word *bad*, then it, too, must carry some sort of negative sentiment. We *propogate* the negativity or positivity from the seed words to other words in the dataset. \n",
    "\n",
    "I'll demonstrate this approach here, but with the **strong caveat** that since this is a relatively small dataset, it probably won't work well. If you decide to take this route, I strongly advise you collect far more data than 5000 Tweets.\n",
    "\n",
    "### Pointwise Mutual Information\n",
    "\n",
    "Pointwise Mutual Information, or PMI, is a measurement of how strongly associated two words are given a corpus and a unit of analysis. It is given by the following formula:\n",
    "\n",
    "$$\n",
    "\\text{PMI}(a, b) = \\text{log}\\frac{P(a, b)}{P(a) \\cdot P(b)}\n",
    "$$\n",
    "\n",
    "Where $P(a, b)$ is the probability that both a and b appear in the same unit of analysis (like a sentence, tweet, or document), while $P(a)$ is the probability that a appears in a unit and $P(b)$ is the probability that b appears in a unit. \n",
    "\n",
    "The intuition behind this formula is that if two words appear together more often than we would expect them to appear by chance, then they are strongly associated. The numerator represents how likely they are to appear together, the denominator represents how often we expect them appear together by chance. \n",
    "\n",
    "If we take Tweets to be our unit of analysis:\n",
    "\n",
    "* $P(a, b)$ is simply the number of Tweets that a and b appear together divided by the total number of tweets\n",
    "* $P(a)$ is the number of Tweets that a appears in, divided by the total number of tweets\n",
    "* $P(b)$ is the number of Tweets that b appears in, divided by the total number of tweets\n",
    "\n",
    "### Semantic Orientation of a Word\n",
    "\n",
    "We start with a set of positive and negative seed words. We'll call the positive seed words $V^{+}$ and the negative seed words $V^{-}$. Let's go ahead and make these sets. \n",
    "\n",
    "$$\n",
    "V^{+} = \\{\\text{good}, \\text{great}, \\text{better}\\} \\\\\n",
    "V^{-} = \\{\\text{bad}, \\text{terrible}, \\text{worse}\\}\n",
    "$$\n",
    "\n",
    "The semantic orientation of a word is simply the sum of its PMIs with the positive seed words, minus the sum of its PMIs with negative seed words. \n",
    "\n",
    "$$\n",
    "\\text{SO}(w) = \\sum_{v^{+} \\in V^{+}} \\text{PMI}(w, v^{+}) - \\sum_{v^{-} \\in V^{-}} \\text{PMI}(w, v^{-})\n",
    "$$\n",
    "\n",
    "Once you have the semantic orientation for all the words in your corpus, you have something similar to the AFINN lexicon - each word has a positive or negative word associated it. From there, you can iterate through all your tweets. \n",
    "\n",
    "The step-by-step process of getting from a corpus and seed words to a list of words with semantic orientations is pretty complex, so I've written a function here that will do the work for you. You can copy and paste this code into your own if you wish to take this approach. \n",
    "\n",
    "The function ``getso`` takes four arguments:\n",
    "\n",
    "* ``seed_pos``: a list containing the positive seed words\n",
    "* ``seed_neg``: a list containing the negative seed words\n",
    "* ``texts``: an iterable containing the raw texts\n",
    "* ``min_df``: an integer, meaning \"minimum document frequency\". This tells the function to ignore all words that appear less than ``min_df`` times in the corpus. This will remove outliers that appear infrequently and therefore are noise.\n",
    "\n",
    "The function returns a dictionary. The keys of the dictionary are words, their corresponding values are the SO calculated for that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getso(seed_pos, seed_neg, texts, min_df):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.preprocessing import binarize\n",
    "    import numpy as np\n",
    "    n = len(texts)\n",
    "    cv = CountVectorizer(min_df=min_df)\n",
    "    vector = cv.fit_transform(texts)\n",
    "    rvocab = {v: k for k, v in cv.vocabulary_.items()}\n",
    "    bvector = binarize(vector)\n",
    "    docfreq = np.sum(bvector.toarray(), axis=0)\n",
    "    docfreq = docfreq/n\n",
    "    docfreq = docfreq.reshape(docfreq.shape[0], 1)\n",
    "    com = vector.T.dot(vector).toarray()\n",
    "    np.fill_diagonal(com, 0)\n",
    "    com = com/n\n",
    "    p_single = docfreq.dot(docfreq.T)\n",
    "    pmi = np.divide(com, p_single)\n",
    "    sos = {}\n",
    "    for word, index in cv.vocabulary_.items():\n",
    "        so = 0\n",
    "        for pos in seed_pos:\n",
    "            p_index = cv.vocabulary_[pos]\n",
    "            so += pmi[index, p_index]\n",
    "        for neg in seed_neg:\n",
    "            n_index = cv.vocabulary_[neg]\n",
    "            so -= pmi[index, n_index]\n",
    "        sos[word] = so\n",
    "    return sos  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the seed words and the tweet texts and feed them to this new function. This will result in a dictionary of semantic orientations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_pos = [\"good\", \"great\", \"better\"]\n",
    "seed_neg = [\"bad\", \"terrible\", \"worse\"]\n",
    "texts = df['text']\n",
    "so = getso(seed_pos, seed_neg, texts, min_df=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the semantic orientation of the words \"hillary\", \"trump\", \"economy\" and \"terrorist\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.74509595015\n",
      "0.309027941379\n",
      "-4.85461285009\n",
      "-13.9458884329\n"
     ]
    }
   ],
   "source": [
    "print(so['hillary'])\n",
    "print(so['trump'])\n",
    "print(so['economy'])\n",
    "print(so['terrorist'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"hillary\" and \"economy\" have slightly negative orientations, \"trump\" has a slightly positive one, while \"terrorist\" has a very negative one.\n",
    "\n",
    "Already this should set off some alarm bells as to whether or not we have obtained some effective semantic orientations. We've gotten equal numbers of tweets from both candidates, but our semantic orientations seem slightly biased towards Trump. There are a number of potential reasons for this (included the small amount of data we are using), but one of them might be a tendency for the Trump twitter account to use more adjectives like the seed words we have chosen; therefore, the Trump account influences the semantic orientation outcome slightly more. \n",
    "\n",
    "Here's a look at the most positive and negative words, by semantic orientation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barriers', 23.076923076923077),\n",
       " ('david', 19.230769230769234),\n",
       " ('benefits', 18.641671466722524),\n",
       " ('politics', 17.618200783461127),\n",
       " ('paying', 16.132720495758157),\n",
       " ('facing', 15.384615384615387),\n",
       " ('ourselves', 13.986013986013987),\n",
       " ('wages', 13.784887678692993),\n",
       " ('do', 13.613680347275036),\n",
       " ('deserve', 13.206262763784888),\n",
       " ('than', 13.03075976803669),\n",
       " ('biggest', 13.008824149662459),\n",
       " ('evening', 12.798415311792549),\n",
       " ('press', 12.69122590382109),\n",
       " ('michael', 12.582938164333511),\n",
       " ('stay', 12.302350342782848),\n",
       " ('looking', 12.254336360805459),\n",
       " ('courage', 12.117086453369641),\n",
       " ('grow', 12.117086453369641),\n",
       " ('tuesday', 11.608402707999575),\n",
       " ('rnc', 11.568276684555755),\n",
       " ('face', 11.538461538461538),\n",
       " ('hate', 11.406153410681092),\n",
       " ('spending', 11.25049678976292),\n",
       " ('afternoon', 11.223922008033966),\n",
       " ('higher', 11.117736699132049),\n",
       " ('job', 11.068784712254704),\n",
       " ('children', 11.067585144041205),\n",
       " ('since', 10.82497763083839),\n",
       " ('polls', 10.6869941500645),\n",
       " ('100', 10.210465028328787),\n",
       " ('able', 10.14154718620658),\n",
       " ('problems', 10.097572044474703),\n",
       " ('clear', 9.6153846153846168),\n",
       " ('especially', 9.4216455607688374),\n",
       " ('heading', 9.3667051888908244),\n",
       " ('tampa', 9.3420791095209701),\n",
       " ('democrats', 9.3242810915668279),\n",
       " ('politicians', 9.2647805826100402),\n",
       " ('path', 9.2195883510968919),\n",
       " ('listen', 9.1899251191286595),\n",
       " ('any', 9.0878148400272281),\n",
       " ('apart', 8.9842973563903801),\n",
       " ('profits', 8.8495575221238933),\n",
       " ('wage', 8.8495575221238933),\n",
       " ('congrats', 8.8495575221238933),\n",
       " ('place', 8.7723567177501458),\n",
       " ('honor', 8.704853438095574),\n",
       " ('lead', 8.5801364871132311),\n",
       " ('please', 8.5684270278050167)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted(so.items(), key=lambda x:x[1], reverse=True))[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('judgement', -65.36172161172162),\n",
       " ('tpp', -46.914623837700759),\n",
       " ('getting', -46.800473041297629),\n",
       " ('growth', -46.27529554855137),\n",
       " ('reporting', -41.666666666666671),\n",
       " ('dead', -40.700404743338552),\n",
       " ('china', -38.224318747574571),\n",
       " ('nafta', -35.880729209524304),\n",
       " ('crime', -35.305535201347098),\n",
       " ('victims', -34.722222222222221),\n",
       " ('attacks', -32.738095238095241),\n",
       " ('san', -32.274895646988675),\n",
       " ('voted', -31.918579208402218),\n",
       " ('wrote', -30.885780885780882),\n",
       " ('zero', -29.737379197144584),\n",
       " ('possible', -28.733839987992816),\n",
       " ('mess', -28.694756570862765),\n",
       " ('which', -27.485816210669576),\n",
       " ('held', -26.327838827838832),\n",
       " ('divided', -26.069726733443552),\n",
       " ('anti', -25.635794913640694),\n",
       " ('crazy', -25.248835158010753),\n",
       " ('course', -23.708010335917315),\n",
       " ('income', -22.727272727272727),\n",
       " ('call', -22.588522588522586)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted(so.items(), key=lambda x:x[1]))[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some the negative words do seem to be appropriately marked (dead, crime, victims, attacks, mess), but others seem to again be indicative of a bias towards Trump (due to negative marks for words like \"TPP\" and \"China\"). The words classified as highly positive aren't exactly of high quality. Words like barrier, facing, and David are scored as being highly positive.\n",
    "\n",
    "There are a few of reasons for this unimpressive result:\n",
    "\n",
    "* As I mentioned, for the task of automatic lexicon generation, this is a pretty small dataset. There really isn't enough text here to demonstrate that two given words have a strong relationship. If you use this approach, try to have more than 5000 texts available to you.\n",
    "* The quality of automatic lexicon generation depends very much on the words you choose as positive and negative seeds. I just \"guessed\" words such as \"good\", \"great\", and \"better\" to be positive words, but it would probably be better for you to do a systematic exploration of your data to find some good seed words. Choosing *more* seed words would also help.\n",
    "\n",
    "Assuming you do get a decent lexicon from this automatic process, you can then take a route similar with AFINN; go through each tweet, add up the SO scores for each word in the tweet, and you have one number representing the polarity of the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised vs. Unsupervised\n",
    "\n",
    "Notice that our attempts to identify the sentiment polarity of Tweets has been, up to this point, **entirely unsupervised**. We have had to rely on some sort of *external knowledge base* like AFINN to assign some sort of quantitative measure of polarity. What we have *not* done is take a set of labeled examples, train a classifier on it, and run some evaluations on it. \n",
    "\n",
    "Why?\n",
    "\n",
    "The key problem is *labeled data*. In order to get a bunch of tweets and used supervised learning to classify their sentiment, we need some *labeled data* to train the classifier. But for the task at hand, we don't have any labeled data. \n",
    "\n",
    "Previously, we have used supervised learning methods to predict the species of an Iris, or to predict whether or not a Tweet came from iPhone or Android. In all those scenarios, the labels were provided for us, either automatically (in the case of Twitter, which provides tweet source as a property), or manually (some 19th century botanist both measured the Irises and determined their species). \n",
    "\n",
    "But we don't have that luxury here. We don't have any means to know beforehand whether a Tweet is positive or negative - that's why we're trying to figure out the sentiment in the first place. Lacking a labeled dataset, we have to resort to these rather imprecise unsupervised methods to determine polarity. Since it's unsupervised, it suffers one of the characteristic drawbacks of all unsupervised learning: there's no systematic way to evaluate it. \n",
    "\n",
    "## How to get to supervised learning?\n",
    "\n",
    "Since we're not attempting to predict a feature that is *automatically* provided to us by the Twitter environment, we'll have to find a way to get labeled data if we want to use some form of supervised learning.\n",
    "\n",
    "### Manually Labeling a set of Tweets\n",
    "\n",
    "This involves subsetting your tweets and manually identifying them as positive or negative. This is extremely difficult and tedious work, but someone eventually has to do it. It is best practice to have multiple people annotate tweets and then compare the results. The degree to which the results of different coders match is called \"Inter-rate Reliability\", and usually only manual annotations with high IRR are considered acceptable for publication. Another route to take is to hire a service such as Amazon Mechanical Turk to annotate the tweets, \"crowdsourcing\" the work. If you have enough Turkers do the work, then you can measure the IRR among them.\n",
    "\n",
    "### Using an existing dataset\n",
    "\n",
    "If you're lucky, someone has already labeled a dataset relevant to your research interest. Assuming they have provided many samples with high-quality annotations, you can simply train a classifier on their data, and then use that classifier to classify your data.\n",
    "\n",
    "### Find something in your data to serve as a proxy for sentiment\n",
    "\n",
    "If neither of the above two options are available to you, one alternative is to find some indicator in the data you *do* have to serve as an unambiguous proxy for positive or negative sentiment. For example, you could operate with the assumption that every tweet that contains a smiling emoticon, \":)\", is a positive tweet, while one containing a sad emoticon, \":(\", is a negative one. You could train a classifier to classify this subset of tweets, and then use that classifier to classify the rest of your tweets that did not have such emoticons.\n",
    "\n",
    "# Supervised Learning Example\n",
    "\n",
    "Here, we're going to train a sentiment classifier using a combination of approaches 2 and 3 described above. \n",
    "\n",
    "Sentiment140 is a classifier that allows you discover teh sentiment of a brand, product, or topic on Twitter. The designers of the classifier used a dataset containing 1,600,000 tweets to train it. All the tweets in this dataset end in either a smiling emoticon \":)\" or a negative emoticon \":(\". Positive tweets were assigned a label of 4, negative tweets a label of 0. \n",
    "\n",
    "The designers have graciously made their training data public, so what we'll do is use their training data - and the labels that come with them - to train a classifier, and *then* use that classifier to make predictions on new, unseen data: namely, our Clinton/Trump tweets.  \n",
    "\n",
    "I've put their data in .csv format on Canvas. Here, I use Pandas to import them as a data frame into the Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", names=[\"polarity\", \"tid\", \"date\", \"query\", \"user\", \"text\"], encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tid</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811594</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>coZZ</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811795</td>\n",
       "      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>2Hood4Hollywood</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812025</td>\n",
       "      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812416</td>\n",
       "      <td>Mon Apr 06 22:20:16 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>erinx3leannexo</td>\n",
       "      <td>spring break in plain city... it's snowing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812579</td>\n",
       "      <td>Mon Apr 06 22:20:17 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>pardonlauren</td>\n",
       "      <td>I just re-pierced my ears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812723</td>\n",
       "      <td>Mon Apr 06 22:20:19 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TLeC</td>\n",
       "      <td>@caregiving I couldn't bear to watch it.  And ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812771</td>\n",
       "      <td>Mon Apr 06 22:20:19 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>robrobbierobert</td>\n",
       "      <td>@octolinz16 It it counts, idk why I did either...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812784</td>\n",
       "      <td>Mon Apr 06 22:20:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bayofwolves</td>\n",
       "      <td>@smarrison i would've been the first, but i di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812799</td>\n",
       "      <td>Mon Apr 06 22:20:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>HairByJess</td>\n",
       "      <td>@iamjazzyfizzle I wish I got to watch it with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812964</td>\n",
       "      <td>Mon Apr 06 22:20:22 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lovesongwriter</td>\n",
       "      <td>Hollis' death scene will hurt me severely to w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813137</td>\n",
       "      <td>Mon Apr 06 22:20:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>armotley</td>\n",
       "      <td>about to file taxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813579</td>\n",
       "      <td>Mon Apr 06 22:20:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>starkissed</td>\n",
       "      <td>@LettyA ahh ive always wanted to see rent  lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813782</td>\n",
       "      <td>Mon Apr 06 22:20:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>gi_gi_bee</td>\n",
       "      <td>@FakerPattyPattz Oh dear. Were you drinking ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813985</td>\n",
       "      <td>Mon Apr 06 22:20:37 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>quanvu</td>\n",
       "      <td>@alydesigns i was out most of the day so didn'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813992</td>\n",
       "      <td>Mon Apr 06 22:20:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>swinspeedx</td>\n",
       "      <td>one of my friend called me, and asked to meet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>1467814119</td>\n",
       "      <td>Mon Apr 06 22:20:40 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>cooliodoc</td>\n",
       "      <td>@angry_barista I baked you a cake but I ated it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>1467814180</td>\n",
       "      <td>Mon Apr 06 22:20:40 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>viJILLante</td>\n",
       "      <td>this week is not going as i had hoped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>1467814192</td>\n",
       "      <td>Mon Apr 06 22:20:41 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Ljelli3166</td>\n",
       "      <td>blagh class at 8 tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>1467814438</td>\n",
       "      <td>Mon Apr 06 22:20:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ChicagoCubbie</td>\n",
       "      <td>I hate when I have to call and wake people up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>1467814783</td>\n",
       "      <td>Mon Apr 06 22:20:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>KatieAngell</td>\n",
       "      <td>Just going to cry myself to sleep after watchi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>1467814883</td>\n",
       "      <td>Mon Apr 06 22:20:52 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>gagoo</td>\n",
       "      <td>im sad now  Miss.Lilly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>1467815199</td>\n",
       "      <td>Mon Apr 06 22:20:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>abel209</td>\n",
       "      <td>ooooh.... LOL  that leslie.... and ok I won't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>1467815753</td>\n",
       "      <td>Mon Apr 06 22:21:04 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BaptisteTheFool</td>\n",
       "      <td>Meh... Almost Lover is the exception... this t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599970</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578196</td>\n",
       "      <td>Tue Jun 16 08:38:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>adbillingsley</td>\n",
       "      <td>Thanks @eastwestchic &amp;amp; @wangyip Thanks! Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599971</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578237</td>\n",
       "      <td>Tue Jun 16 08:38:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>gekkko</td>\n",
       "      <td>@marttn thanks Martin. not the most imaginativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599972</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578269</td>\n",
       "      <td>Tue Jun 16 08:38:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>millerslab</td>\n",
       "      <td>@MikeJonesPhoto Congrats Mike  Way to go!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599973</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578319</td>\n",
       "      <td>Tue Jun 16 08:38:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>luckygeorgeblog</td>\n",
       "      <td>http://twitpic.com/7jp4n - OMG! Office Space.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599974</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578345</td>\n",
       "      <td>Tue Jun 16 08:38:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Kristah_Diggs</td>\n",
       "      <td>@yrclndstnlvr ahaha nooo you were just away fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599975</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578347</td>\n",
       "      <td>Tue Jun 16 08:38:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>CoachChic</td>\n",
       "      <td>@BizCoachDeb  Hey, I'm baack! And, thanks so m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599976</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578348</td>\n",
       "      <td>Tue Jun 16 08:38:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>serianna</td>\n",
       "      <td>@mattycus Yeah, my conscience would be clear i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599977</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578386</td>\n",
       "      <td>Tue Jun 16 08:38:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TeamUKskyvixen</td>\n",
       "      <td>@MayorDorisWolfe Thats my girl - dishing out t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599978</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578395</td>\n",
       "      <td>Tue Jun 16 08:38:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LaurenMoo10</td>\n",
       "      <td>@shebbs123 i second that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599979</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578576</td>\n",
       "      <td>Tue Jun 16 08:38:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>angel_sammy04</td>\n",
       "      <td>In the garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599980</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578679</td>\n",
       "      <td>Tue Jun 16 08:38:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>puchal_ek</td>\n",
       "      <td>@myheartandmind jo jen by nemuselo zrovna tÃ© ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599981</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578716</td>\n",
       "      <td>Tue Jun 16 08:38:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>youtubelatest</td>\n",
       "      <td>Another Commenting Contest! [;: Yay!!!  http:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599982</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578739</td>\n",
       "      <td>Tue Jun 16 08:38:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Mandi_Davenport</td>\n",
       "      <td>@thrillmesoon i figured out how to see my twee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599983</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578758</td>\n",
       "      <td>Tue Jun 16 08:38:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xoAurixo</td>\n",
       "      <td>@oxhot theri tomorrow, drinking coffee, talkin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599984</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578847</td>\n",
       "      <td>Tue Jun 16 08:38:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RobFoxKerr</td>\n",
       "      <td>You heard it here first -- We're having a girl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599985</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578982</td>\n",
       "      <td>Tue Jun 16 08:38:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LISKFEST</td>\n",
       "      <td>if ur the lead singer in a band, beware fallin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599986</th>\n",
       "      <td>4</td>\n",
       "      <td>2193579087</td>\n",
       "      <td>Tue Jun 16 08:38:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>marhgil</td>\n",
       "      <td>@tarayqueen too much ads on my blog.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599987</th>\n",
       "      <td>4</td>\n",
       "      <td>2193579092</td>\n",
       "      <td>Tue Jun 16 08:38:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>cathriiin</td>\n",
       "      <td>@La_r_a NEVEER  I think that you both will get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599988</th>\n",
       "      <td>4</td>\n",
       "      <td>2193579191</td>\n",
       "      <td>Tue Jun 16 08:38:59 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tellman</td>\n",
       "      <td>@Roy_Everitt ha- good job. that's right - we g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599989</th>\n",
       "      <td>4</td>\n",
       "      <td>2193579211</td>\n",
       "      <td>Tue Jun 16 08:38:59 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jazzstixx</td>\n",
       "      <td>@Ms_Hip_Hop im glad ur doing well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599990</th>\n",
       "      <td>4</td>\n",
       "      <td>2193579249</td>\n",
       "      <td>Tue Jun 16 08:38:59 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>razzberry5594</td>\n",
       "      <td>WOOOOO! Xbox is back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599991</th>\n",
       "      <td>4</td>\n",
       "      <td>2193579284</td>\n",
       "      <td>Tue Jun 16 08:38:59 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AgustinaP</td>\n",
       "      <td>@rmedina @LaTati Mmmm  That sounds absolutely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599992</th>\n",
       "      <td>4</td>\n",
       "      <td>2193579434</td>\n",
       "      <td>Tue Jun 16 08:39:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sdancingsteph</td>\n",
       "      <td>ReCoVeRiNg FrOm ThE lOnG wEeKeNd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599993</th>\n",
       "      <td>4</td>\n",
       "      <td>2193579477</td>\n",
       "      <td>Tue Jun 16 08:39:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ChloeAmisha</td>\n",
       "      <td>@SCOOBY_GRITBOYS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>4</td>\n",
       "      <td>2193579489</td>\n",
       "      <td>Tue Jun 16 08:39:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>EvolveTom</td>\n",
       "      <td>@Cliff_Forster Yeah, that does work better tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         polarity         tid                          date     query  \\\n",
       "0               0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1               0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2               0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3               0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4               0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "5               0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "6               0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "7               0  1467811594  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "8               0  1467811795  Mon Apr 06 22:20:05 PDT 2009  NO_QUERY   \n",
       "9               0  1467812025  Mon Apr 06 22:20:09 PDT 2009  NO_QUERY   \n",
       "10              0  1467812416  Mon Apr 06 22:20:16 PDT 2009  NO_QUERY   \n",
       "11              0  1467812579  Mon Apr 06 22:20:17 PDT 2009  NO_QUERY   \n",
       "12              0  1467812723  Mon Apr 06 22:20:19 PDT 2009  NO_QUERY   \n",
       "13              0  1467812771  Mon Apr 06 22:20:19 PDT 2009  NO_QUERY   \n",
       "14              0  1467812784  Mon Apr 06 22:20:20 PDT 2009  NO_QUERY   \n",
       "15              0  1467812799  Mon Apr 06 22:20:20 PDT 2009  NO_QUERY   \n",
       "16              0  1467812964  Mon Apr 06 22:20:22 PDT 2009  NO_QUERY   \n",
       "17              0  1467813137  Mon Apr 06 22:20:25 PDT 2009  NO_QUERY   \n",
       "18              0  1467813579  Mon Apr 06 22:20:31 PDT 2009  NO_QUERY   \n",
       "19              0  1467813782  Mon Apr 06 22:20:34 PDT 2009  NO_QUERY   \n",
       "20              0  1467813985  Mon Apr 06 22:20:37 PDT 2009  NO_QUERY   \n",
       "21              0  1467813992  Mon Apr 06 22:20:38 PDT 2009  NO_QUERY   \n",
       "22              0  1467814119  Mon Apr 06 22:20:40 PDT 2009  NO_QUERY   \n",
       "23              0  1467814180  Mon Apr 06 22:20:40 PDT 2009  NO_QUERY   \n",
       "24              0  1467814192  Mon Apr 06 22:20:41 PDT 2009  NO_QUERY   \n",
       "25              0  1467814438  Mon Apr 06 22:20:44 PDT 2009  NO_QUERY   \n",
       "26              0  1467814783  Mon Apr 06 22:20:50 PDT 2009  NO_QUERY   \n",
       "27              0  1467814883  Mon Apr 06 22:20:52 PDT 2009  NO_QUERY   \n",
       "28              0  1467815199  Mon Apr 06 22:20:56 PDT 2009  NO_QUERY   \n",
       "29              0  1467815753  Mon Apr 06 22:21:04 PDT 2009  NO_QUERY   \n",
       "...           ...         ...                           ...       ...   \n",
       "1599970         4  2193578196  Tue Jun 16 08:38:54 PDT 2009  NO_QUERY   \n",
       "1599971         4  2193578237  Tue Jun 16 08:38:54 PDT 2009  NO_QUERY   \n",
       "1599972         4  2193578269  Tue Jun 16 08:38:54 PDT 2009  NO_QUERY   \n",
       "1599973         4  2193578319  Tue Jun 16 08:38:55 PDT 2009  NO_QUERY   \n",
       "1599974         4  2193578345  Tue Jun 16 08:38:55 PDT 2009  NO_QUERY   \n",
       "1599975         4  2193578347  Tue Jun 16 08:38:55 PDT 2009  NO_QUERY   \n",
       "1599976         4  2193578348  Tue Jun 16 08:38:55 PDT 2009  NO_QUERY   \n",
       "1599977         4  2193578386  Tue Jun 16 08:38:55 PDT 2009  NO_QUERY   \n",
       "1599978         4  2193578395  Tue Jun 16 08:38:55 PDT 2009  NO_QUERY   \n",
       "1599979         4  2193578576  Tue Jun 16 08:38:57 PDT 2009  NO_QUERY   \n",
       "1599980         4  2193578679  Tue Jun 16 08:38:56 PDT 2009  NO_QUERY   \n",
       "1599981         4  2193578716  Tue Jun 16 08:38:57 PDT 2009  NO_QUERY   \n",
       "1599982         4  2193578739  Tue Jun 16 08:38:57 PDT 2009  NO_QUERY   \n",
       "1599983         4  2193578758  Tue Jun 16 08:38:57 PDT 2009  NO_QUERY   \n",
       "1599984         4  2193578847  Tue Jun 16 08:38:57 PDT 2009  NO_QUERY   \n",
       "1599985         4  2193578982  Tue Jun 16 08:38:58 PDT 2009  NO_QUERY   \n",
       "1599986         4  2193579087  Tue Jun 16 08:38:58 PDT 2009  NO_QUERY   \n",
       "1599987         4  2193579092  Tue Jun 16 08:38:58 PDT 2009  NO_QUERY   \n",
       "1599988         4  2193579191  Tue Jun 16 08:38:59 PDT 2009  NO_QUERY   \n",
       "1599989         4  2193579211  Tue Jun 16 08:38:59 PDT 2009  NO_QUERY   \n",
       "1599990         4  2193579249  Tue Jun 16 08:38:59 PDT 2009  NO_QUERY   \n",
       "1599991         4  2193579284  Tue Jun 16 08:38:59 PDT 2009  NO_QUERY   \n",
       "1599992         4  2193579434  Tue Jun 16 08:39:00 PDT 2009  NO_QUERY   \n",
       "1599993         4  2193579477  Tue Jun 16 08:39:00 PDT 2009  NO_QUERY   \n",
       "1599994         4  2193579489  Tue Jun 16 08:39:00 PDT 2009  NO_QUERY   \n",
       "1599995         4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996         4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997         4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998         4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999         4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "5               joy_wolf                      @Kwesidei not the whole crew   \n",
       "6                mybirch                                        Need a hug   \n",
       "7                   coZZ  @LOLTrish hey  long time no see! Yes.. Rains a...  \n",
       "8        2Hood4Hollywood               @Tatiana_K nope they didn't have it   \n",
       "9                mimismo                          @twittera que me muera ?   \n",
       "10        erinx3leannexo        spring break in plain city... it's snowing   \n",
       "11          pardonlauren                         I just re-pierced my ears   \n",
       "12                  TLeC  @caregiving I couldn't bear to watch it.  And ...  \n",
       "13       robrobbierobert  @octolinz16 It it counts, idk why I did either...  \n",
       "14           bayofwolves  @smarrison i would've been the first, but i di...  \n",
       "15            HairByJess  @iamjazzyfizzle I wish I got to watch it with ...  \n",
       "16        lovesongwriter  Hollis' death scene will hurt me severely to w...  \n",
       "17              armotley                               about to file taxes   \n",
       "18            starkissed  @LettyA ahh ive always wanted to see rent  lov...  \n",
       "19             gi_gi_bee  @FakerPattyPattz Oh dear. Were you drinking ou...  \n",
       "20                quanvu  @alydesigns i was out most of the day so didn'...  \n",
       "21            swinspeedx  one of my friend called me, and asked to meet ...  \n",
       "22             cooliodoc   @angry_barista I baked you a cake but I ated it   \n",
       "23            viJILLante             this week is not going as i had hoped   \n",
       "24            Ljelli3166                         blagh class at 8 tomorrow   \n",
       "25         ChicagoCubbie     I hate when I have to call and wake people up   \n",
       "26           KatieAngell  Just going to cry myself to sleep after watchi...  \n",
       "27                 gagoo                             im sad now  Miss.Lilly  \n",
       "28               abel209  ooooh.... LOL  that leslie.... and ok I won't ...  \n",
       "29       BaptisteTheFool  Meh... Almost Lover is the exception... this t...  \n",
       "...                  ...                                                ...  \n",
       "1599970    adbillingsley  Thanks @eastwestchic &amp; @wangyip Thanks! Th...  \n",
       "1599971           gekkko  @marttn thanks Martin. not the most imaginativ...  \n",
       "1599972       millerslab          @MikeJonesPhoto Congrats Mike  Way to go!  \n",
       "1599973  luckygeorgeblog  http://twitpic.com/7jp4n - OMG! Office Space.....  \n",
       "1599974    Kristah_Diggs  @yrclndstnlvr ahaha nooo you were just away fr...  \n",
       "1599975        CoachChic  @BizCoachDeb  Hey, I'm baack! And, thanks so m...  \n",
       "1599976         serianna  @mattycus Yeah, my conscience would be clear i...  \n",
       "1599977   TeamUKskyvixen  @MayorDorisWolfe Thats my girl - dishing out t...  \n",
       "1599978      LaurenMoo10                          @shebbs123 i second that   \n",
       "1599979    angel_sammy04                                     In the garden   \n",
       "1599980        puchal_ek  @myheartandmind jo jen by nemuselo zrovna tÃ© ...  \n",
       "1599981    youtubelatest  Another Commenting Contest! [;: Yay!!!  http:/...  \n",
       "1599982  Mandi_Davenport  @thrillmesoon i figured out how to see my twee...  \n",
       "1599983         xoAurixo  @oxhot theri tomorrow, drinking coffee, talkin...  \n",
       "1599984       RobFoxKerr  You heard it here first -- We're having a girl...  \n",
       "1599985         LISKFEST  if ur the lead singer in a band, beware fallin...  \n",
       "1599986          marhgil              @tarayqueen too much ads on my blog.   \n",
       "1599987        cathriiin  @La_r_a NEVEER  I think that you both will get...  \n",
       "1599988          tellman  @Roy_Everitt ha- good job. that's right - we g...  \n",
       "1599989        jazzstixx                 @Ms_Hip_Hop im glad ur doing well   \n",
       "1599990    razzberry5594                              WOOOOO! Xbox is back   \n",
       "1599991        AgustinaP  @rmedina @LaTati Mmmm  That sounds absolutely ...  \n",
       "1599992    sdancingsteph                  ReCoVeRiNg FrOm ThE lOnG wEeKeNd   \n",
       "1599993      ChloeAmisha                                  @SCOOBY_GRITBOYS   \n",
       "1599994        EvolveTom  @Cliff_Forster Yeah, that does work better tha...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll represent the tweet texts in vector space format using sklearn's TFIDF vectorizer. I call the named argument ``min_df`` with a value of 10 to tell the vectorizer to ignore all words that occur in fewer than 10 tweets. We have a lot of tweets, so words that occur so infrequently don't have much statistical weight but also would increase the dimensionality of the vectors by a lot, so we ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=50)\n",
    "X = tv.fit_transform(training['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 13451)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our X matrix has 1,600,000 rows (one for each sample) and 43,448 columns, meaning there are 43,448 unique words in this dataset that occur in 10 or more tweets. \n",
    "\n",
    "Next, I get the polarity column from the data to serve as our labels in the variable ``y``. Just in case the data is stored in a specific order, I shuffle it using ``shuffle``. \n",
    "\n",
    "Finally, training a model on 1,600,000 samples would take a long time. I'll only get 500,000 of them to shorten the training time. I'm only doing this here for instructional purposes, otherwise, you'd want to train your model on as many data points as possible, even if it takes a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = training['polarity']\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X, y = shuffle(X, y)\n",
    "\n",
    "X, y = X[:50000], y[:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I continue in our familiar fashion. I'll train a linear SVM on the data. I'll split the data into training and test sets because I want to get an idea of how well the classifier works generally. We'll use the held-out test data to evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3057)\n",
    "clf = SVC(kernel=\"linear\", verbose=3)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77424000000000004"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM gets an accuracy of 77% on the held out data, so it's a decently good classifier.\n",
    "\n",
    "Now, we take this **already trained classifier** and have it **make predictions on our data**.\n",
    "\n",
    "First, we have to conver our data into vector representations. These have to be **the same type of vector representations used to train the classifier**. How do we do this?\n",
    "\n",
    "Simple! We use the same TFIDF vectorizer we used on the training data. Remember, we saved it in a variable called ``tv``. **Important:** this time, we use the method ``.transform()`` **instead** of ``.fit_transform()``. This is because the TFIDF Vectorizer has already been fit to the training data. We want only to transform our Tweet texts into a TF-IDF representation and not fit the vectorizer to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_tfidf = tv.transform(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we make predictions using our already-trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_pred = clf.predict(tweet_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the predictions of the classifier to the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sent_pred\"] = tweet_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I print out the first 50 Tweets and the label assigned by the classifier. Remember, 0 means negative and 4 means positive. It's not perfect, but it's decent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Remember, don\\'t believe \"sources said\" by the VERY dishonest media. If they don\\'t name the sources, the sources don\\'t exist.', 0)\n",
      "**************************************************\n",
      "('Did Crooked Hillary help disgusting (check out sex tape and past) Alicia M become a U.S. citizen so she could use her in the debate?', 4)\n",
      "**************************************************\n",
      "('Using Alicia M in the debate as a paragon of virtue just shows that Crooked Hillary suffers from BAD JUDGEMENT! Hillary was set up by a con.', 4)\n",
      "**************************************************\n",
      "('Wow, Crooked Hillary was duped and used by my worst Miss U. Hillary floated her as an \"angel\" without checking her past, which is terrible!', 0)\n",
      "**************************************************\n",
      "('Anytime you see a story about me or my campaign saying \"sources said,\" DO NOT believe it. There are no sources, they are just made up lies!', 0)\n",
      "**************************************************\n",
      "('Wow, did you see how badly @CNN (Clinton News Network) is doing in the ratings. With people like @donlemon, who could expect any more?', 4)\n",
      "**************************************************\n",
      "('While Hillary profits off the rigged system, I am fighting for you! Remember the simple phrase: #FollowTheMoney\\x85 https://t.co/8mVInc82E9', 4)\n",
      "**************************************************\n",
      "('Thank you for joining me this afternoon, New Hampshire! Will be back soon. #FollowTheMoney\\nSpeech transcript:\\x85 https://t.co/VtUkDgF4vs', 4)\n",
      "**************************************************\n",
      "('Join me in Manheim, Pennsylvania on Saturday at 7pm! #TrumpRally\\nTickets: https://t.co/ADOGW34ctF https://t.co/LNWQZ9yUJy', 4)\n",
      "**************************************************\n",
      "(\"My condolences to those involved in today's horrible accident in NJ and my deepest gratitude to all of the amazing first responders.\", 0)\n",
      "**************************************************\n",
      "('Will be in Novi, Michigan this Friday at 5:00pm. Join the MOVEMENT! Tickets available at: https://t.co/Q6APf0ZFYA\\x85 https://t.co/6WAyO9eQHN', 4)\n",
      "**************************************************\n",
      "(\"Join me in Bedford, New Hampshire- tomorrow at 3:00pm. Can't wait to see everyone! #AmericaFirst #MAGA\\x85 https://t.co/oeOJFAS7it\", 4)\n",
      "**************************************************\n",
      "('Thank you Waukesha, Wisconsin! \\nFull transcript of my speech, #FollowTheMoney:\\nhttps://t.co/Xb1yyDSNNf https://t.co/WdKK6nJCZW', 0)\n",
      "**************************************************\n",
      "('Joining @oreillyfactor from Waukesha, Wisconsin - now, live! Enjoy!', 4)\n",
      "**************************************************\n",
      "('Join me live in Waukesha, Wisconsin for an 8pmE rally! #AmericaFirst #MAGA\\nhttps://t.co/G8kGLSFy6S', 4)\n",
      "**************************************************\n",
      "('Thank you Council Bluffs, Iowa! Will be back soon. Remember- everything you need to know about Hillary -- just\\x85 https://t.co/45kIHxdX83', 4)\n",
      "**************************************************\n",
      "('RT @TeamTrump: \"She put the office of Sec of State up for sale. If she ever got the chance, she\\x92d put the Oval Office up for sale too.\" #Fo\\x85', 4)\n",
      "**************************************************\n",
      "('An honor to meet with the Polish American Congress in Chicago this morning! #ImWithYou \\nVideo:\\x85 https://t.co/lBFHoWRqox', 4)\n",
      "**************************************************\n",
      "('Melania and I extend our deepest condolences to the family of Shimon Peres...https://t.co/xeGYL2IzUP', 0)\n",
      "**************************************************\n",
      "('Join me in Council Bluffs, Iowa- today at 3pm! #MakeAmericaGreatAgain \\nTickets: https://t.co/iRL3xh37gF', 4)\n",
      "**************************************************\n",
      "('Every on-line poll, Time Magazine, Drudge etc., has me winning the debate. Thank you to Fox &amp; Friends for so reporting!', 4)\n",
      "**************************************************\n",
      "('My supporters are the best! $18 million from hard-working people who KNOW what we can be again! Shatter the record: https://t.co/8ZHGyOth0f', 0)\n",
      "**************************************************\n",
      "('Unbelievable evening in Melbourne, Florida w/ 15,000 supporters- and an additional 12,000 who could not get in. Tha\\x85 https://t.co/VU5wh2zXBU', 4)\n",
      "**************************************************\n",
      "('Join me for a 3pm rally - tomorrow at the Mid-America Center in Council Bluffs, Iowa! Tickets:\\x85 https://t.co/dfzsbICiXc', 4)\n",
      "**************************************************\n",
      "('Once again, we will have a government of, by and for the people. Join the MOVEMENT today! https://t.co/lWjYDbPHav https://t.co/uYwJrtZkAe', 4)\n",
      "**************************************************\n",
      "(\"RT @GOP: On National #VoterRegistrationDay, make sure you're registered to vote so we can #MakeAmericaGreatAgain https://t.co/GKcaLkx8C8 ht\\x85\", 4)\n",
      "**************************************************\n",
      "(\"Hillary Clinton's Campaign Continues To Make False Claims About Foundation Disclosure: \\nhttps://t.co/zhkEfUouHH\", 4)\n",
      "**************************************************\n",
      "(\"'CNBC, Time magazine online polls say Donald Trump won the first presidential debate' via @WashTimes. #MAGA\\nhttps://t.co/PGimqYKPoJ\", 4)\n",
      "**************************************************\n",
      "('Great afternoon in Little Havana with Hispanic community leaders. Thank you for your support! #ImWithYou https://t.co/vxWZ2tyJTF', 4)\n",
      "**************************************************\n",
      "('In the last 24 hrs. we have raised over $13M from online donations and National Call Day, and we\\x92re still going! Thank you America! #MAGA', 4)\n",
      "**************************************************\n",
      "(\"Well, now they're saying that I not only won the NBC Presidential Forum, but last night the big debate. Nice!\", 0)\n",
      "**************************************************\n",
      "('Thank you for your endorsement, @GovernorSununu. #MAGA \\nhttps://t.co/8BEeQPsuyd', 4)\n",
      "**************************************************\n",
      "('Such a great honor. Final debate polls are in - and the MOVEMENT wins!\\n#AmericaFirst #MAGA #ImWithYou\\x85 https://t.co/DV1BKMwHEM', 4)\n",
      "**************************************************\n",
      "(\"'U.S. Murders Increased 10.8% in 2015' via @WSJ: https://t.co/CIJMQJhLqp\", 4)\n",
      "**************************************************\n",
      "('Thank you! #TrumpWon #MAGA \\nhttps://t.co/a5rr1i38km', 4)\n",
      "**************************************************\n",
      "(\"Hillary's been failing for 30 years in not getting the job done - it will never change.\", 0)\n",
      "**************************************************\n",
      "(\"'True blue-collar billionaire Donald Trump shows Hillary Clinton is out of touch' https://t.co/NHO1OicfVm\", 0)\n",
      "**************************************************\n",
      "('The #1 trend on Twitter right now is #TrumpWon - thank you!', 4)\n",
      "**************************************************\n",
      "('I won every poll from last nights Presidential Debate - except for the little watched @CNN poll.', 4)\n",
      "**************************************************\n",
      "(\"'How Trump won over a bar full of undecideds and Democrats'\\nhttps://t.co/WWO39kxn8Y\", 0)\n",
      "**************************************************\n",
      "(\"I really enjoyed the debate last night.Crooked Hillary says she is going to do so many things.Why hasn't she done them in her last 30 years?\", 0)\n",
      "**************************************************\n",
      "('Great debate poll numbers - I will be on @foxandfriends at 7:00 to discuss. Enjoy!', 4)\n",
      "**************************************************\n",
      "('Thank you! Four new #DebateNight polls with the MOVEMENT winning. Together, we will MAKE AMERICA SAFE &amp; GREAT AGAIN\\x85 https://t.co/39FCnUf8Pb', 4)\n",
      "**************************************************\n",
      "(\".@DRUDGE_REPORT's First Presidential Debate Poll:\\nTrump: 80%\\nClinton: 20%\\nJoin the MOVEMENT today &amp; lets #MAGA!\\x85 https://t.co/B12lgC97tn\", 4)\n",
      "**************************************************\n",
      "('Thank you! CNBC #DebateNight poll with over 400,000 votes. \\nTrump 61%\\nClinton 39%\\n#AmericaFirst #ImWithYou\\x85 https://t.co/MJ3NwA98op', 4)\n",
      "**************************************************\n",
      "('TIME #DebateNight poll - over 800,000 votes. Thank you! \\n#AmericaFirst #MAGA https://t.co/bTPX9E0wKu', 4)\n",
      "**************************************************\n",
      "('.@newtgingrich just said \"a historic victory for Trump.\" NICE!', 4)\n",
      "**************************************************\n",
      "(\"Wow, did great in the debate polls (except for @CNN - which I don't watch). Thank you!\", 4)\n",
      "**************************************************\n",
      "('Thank you Governor @TerryBranstad! \\n#AmericaFirst #Debates2016 https://t.co/yIeZctdQy8', 4)\n",
      "**************************************************\n",
      "('Thank you Governor @Mike_Pence!\\nLets MAKE AMERICA SAFE AND GREAT AGAIN with the American people. \\n#AmericaFirst\\x85 https://t.co/6k7qP9X8nC', 4)\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.head(50).iterrows():\n",
    "    print((row['text'], row['sent_pred']))\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
