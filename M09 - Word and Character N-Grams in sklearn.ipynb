{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work with a small amount of fake data to illustrate, but nothing in principle prevents this from working with any set of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The quick brown fox jumps behind the lazy dog.\",\n",
    "    \"The lazy brown fox leaps beyond the sleeping dog.\",\n",
    "    \"This lazy dog has a Twitter account that the fox subscribes to.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the count vectorizer as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, we can make texts turn into a count vector by initializing a Count Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "vectors = cv.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 19)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 2, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vectors.shape)\n",
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information about *which element in a vector* corresponds to *which word* is in the vectorizer object, as the attribute ``vocabulary_``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fox': 5, 'that': 14, 'lazy': 8, 'beyond': 2, 'over': 10, 'jumps': 7, 'this': 16, 'subscribes': 13, 'has': 6, 'leaps': 9, 'brown': 3, 'quick': 11, 'account': 0, 'the': 15, 'sleeping': 12, 'behind': 1, 'dog': 4, 'to': 17, 'twitter': 18}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 0th element of the first count vector has a value of 0, we know the word \"account\" appears in it 0 times. The 1st element of the first count vector has a value of 1, so we know \"brown\" appears in it 1 time.\n",
    "\n",
    "Sometimes, we want to modify how the Count Vectorizer does its vectorization. We can do this by *passing parameters to the factory method during initialization*.\n",
    "\n",
    "For example, we have seen in some readings that **presence vectors** do better than **count vectors**. In a **presence vector** the value 1 indicates that the word is in the text, 0 means it's absence. We don't care how many times a word appears, though.\n",
    "\n",
    "The factory method has a named argument called ``binary`` that, if you set to True, will produce presence vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 19)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "vectors = cv.fit_transform(texts)\n",
    "print(vectors.shape)\n",
    "vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the sample sentences, some words appeared twice in the same text, and therefore are represented by a 2 in the Count Vectors. However, the presence vectors only have values of 0 and 1.\n",
    "\n",
    "# N-Grams\n",
    "\n",
    "As we've discussed before, an n-gram is a combination of n consecutive words. In the first sentence, (\"the\", \"quick\") is a bigram, and (\"quick\", \"brown\") is a bigram. Considering n-grams *preserves some of the information in the order of the words*. For example, with a bigram the classifier can know not just that the sentence had \"quick\" and \"brown\", but that they occurred together.\n",
    "\n",
    "However, sometimes this comes at a cost: since there are *many combinations of 2 words*, the length of the vector becomes *much longer*. This applies in even greater magnitude if you consider 3-grams or 4-grams and so on. On the other hand, the data might become *very sparse* since combinations of words can be more rare than single words.\n",
    "\n",
    "The count vector factory method has a parameter called ngram_range that takes into consideration which n-grams to count. The default value is ``(1, 1)``, which means it considers only unigrams. \n",
    "\n",
    "Here's an example where we consider only bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 25)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2))\n",
    "bigram_vectors = cv.fit_transform(texts)\n",
    "print(bigram_vectors.shape)\n",
    "bigram_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the dimensionality has increased. There are 19 unique words, but there are 25 unique bigrams.\n",
    "\n",
    "If we peek into the vocabulary, we can see that the algorithm is counting combinations of two words, instead of single words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'account that': 0,\n",
       " 'behind the': 1,\n",
       " 'beyond the': 2,\n",
       " 'brown fox': 3,\n",
       " 'dog has': 4,\n",
       " 'fox jumps': 5,\n",
       " 'fox leaps': 6,\n",
       " 'fox subscribes': 7,\n",
       " 'has twitter': 8,\n",
       " 'jumps behind': 9,\n",
       " 'jumps over': 10,\n",
       " 'lazy brown': 11,\n",
       " 'lazy dog': 12,\n",
       " 'leaps beyond': 13,\n",
       " 'over the': 14,\n",
       " 'quick brown': 15,\n",
       " 'sleeping dog': 16,\n",
       " 'subscribes to': 17,\n",
       " 'that the': 18,\n",
       " 'the fox': 19,\n",
       " 'the lazy': 20,\n",
       " 'the quick': 21,\n",
       " 'the sleeping': 22,\n",
       " 'this lazy': 23,\n",
       " 'twitter account': 24}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of a vectorizer that considers *both* unigrams and bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 44)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 2))\n",
    "ngram_vectors = cv.fit_transform(texts)\n",
    "print(ngram_vectors.shape)\n",
    "ngram_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering bigrams and trigrams simultaneously has increased our dimensionality to 44."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'account': 0,\n",
       " 'account that': 1,\n",
       " 'behind': 2,\n",
       " 'behind the': 3,\n",
       " 'beyond': 4,\n",
       " 'beyond the': 5,\n",
       " 'brown': 6,\n",
       " 'brown fox': 7,\n",
       " 'dog': 8,\n",
       " 'dog has': 9,\n",
       " 'fox': 10,\n",
       " 'fox jumps': 11,\n",
       " 'fox leaps': 12,\n",
       " 'fox subscribes': 13,\n",
       " 'has': 14,\n",
       " 'has twitter': 15,\n",
       " 'jumps': 16,\n",
       " 'jumps behind': 17,\n",
       " 'jumps over': 18,\n",
       " 'lazy': 19,\n",
       " 'lazy brown': 20,\n",
       " 'lazy dog': 21,\n",
       " 'leaps': 22,\n",
       " 'leaps beyond': 23,\n",
       " 'over': 24,\n",
       " 'over the': 25,\n",
       " 'quick': 26,\n",
       " 'quick brown': 27,\n",
       " 'sleeping': 28,\n",
       " 'sleeping dog': 29,\n",
       " 'subscribes': 30,\n",
       " 'subscribes to': 31,\n",
       " 'that': 32,\n",
       " 'that the': 33,\n",
       " 'the': 34,\n",
       " 'the fox': 35,\n",
       " 'the lazy': 36,\n",
       " 'the quick': 37,\n",
       " 'the sleeping': 38,\n",
       " 'this': 39,\n",
       " 'this lazy': 40,\n",
       " 'to': 41,\n",
       " 'twitter': 42,\n",
       " 'twitter account': 43}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Tokenization\n",
    "\n",
    "``CountVectorizer`` has a default tokenizer that we have been relying to up to this point. However, there are many situations that call for finer-grained control of the tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annaschneider': 0,\n",
       " 'be': 1,\n",
       " 'hopefully': 2,\n",
       " 'in': 3,\n",
       " 'interested': 4,\n",
       " 'joeschmoe': 5,\n",
       " 'lessons': 6,\n",
       " 'll': 7,\n",
       " 'more': 8,\n",
       " 'on': 9,\n",
       " 'python': 10,\n",
       " 'really': 11,\n",
       " 'rt': 12,\n",
       " 'thanks': 13,\n",
       " 'the': 14,\n",
       " 'there': 15,\n",
       " 'tutorial': 16,\n",
       " 'way': 17,\n",
       " 'your': 18}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweettext = [\n",
    "    \"RT @joeschmoe I'm really interested in your tutorial on #python!\",\n",
    "    \"@annaschneider Thanks! Hopefully there'll be more #python lessons on the way!\"]\n",
    "cv = CountVectorizer()\n",
    "cv.fit_transform(tweettext)\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the ``sklearn`` Count Vectorizer ignored \"RT\" and stripped the @ sign from the mention and the # sign from the retweet. \n",
    "\n",
    "We know that NLTK has a great Tweet Tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " '@joeschmoe',\n",
       " \"I'm\",\n",
       " 'really',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'your',\n",
       " 'tutorial',\n",
       " 'on',\n",
       " '#python',\n",
       " '!']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "tt.tokenize(tweettext[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we do if we want a the CountVectorizer object to use a *different tokenizer* before counting tokens and making count vectors? Again , we turn to a parameter to specify when we use the factory method.\n",
    "\n",
    "This parameter is called ``tokenizer`` and *we pass to it the name of the function that we want to use to tokenize*.\n",
    "\n",
    "Normally, if we're tokenizing a single text with the initialized Tweet tokenizer, we'd type:\n",
    "\n",
    "```python\n",
    "tt.tokenize(sometext)\n",
    "```\n",
    "\n",
    "When we want to pass it to a Count Vectorizer object, we use the parameter ``tokenizer=tt.tokenize``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=tt.tokenize)\n",
    "# CV will now use tt.tokenize() to tokenize a text it receives.\n",
    "tweetvector = cv.fit_transform(tweettext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [2, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetvector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '#python': 1,\n",
       " '@annaschneider': 2,\n",
       " '@joeschmoe': 3,\n",
       " 'be': 4,\n",
       " 'hopefully': 5,\n",
       " \"i'm\": 6,\n",
       " 'in': 7,\n",
       " 'interested': 8,\n",
       " 'lessons': 9,\n",
       " 'more': 10,\n",
       " 'on': 11,\n",
       " 'really': 12,\n",
       " 'rt': 13,\n",
       " 'thanks': 14,\n",
       " 'the': 15,\n",
       " \"there'll\": 16,\n",
       " 'tutorial': 17,\n",
       " 'way': 18,\n",
       " 'your': 19}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Count Vectorizer kep the mentions and hashtags intact, because it used the ``tt.tokenize`` function to split the text strings instead of the default tokenizer.\n",
    "\n",
    "# Character N-Grams\n",
    "\n",
    "A *character n-gram* is a situation where we **count characters instead of words**. This might seem strange at first, but machine learning researchers have found that in some situations, character n-grams provide better performance than word n-grams. \n",
    "\n",
    "To see what a character n-gram looks like, let's turn to ``sklearn``. \n",
    "\n",
    "There is no parameter we can set to look at characters instead of words. However! This is not a problem.\n",
    "\n",
    "If you think about it, doing counts of characters instead of words is just involves a different way of tokenizing the text. Instead of **splitting the words** somehow, we split the characters.\n",
    "\n",
    "We can use our knowledge of the CountVector parameter ``tokenizer`` to take advantage of this. We'll pass it a function that takes a string and splits it into all its characters.\n",
    "\n",
    "In Python, splitting a text string into its constituent characters is actually quite easy: we simply convert the string into a list directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n"
     ]
    }
   ],
   "source": [
    "astring = \"Hello world!\"\n",
    "characters = list(astring)\n",
    "print(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That being said, we can simply pass the ``list`` function directly to the Count Vectorizer's ``tokenizer`` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=list)\n",
    "character_vectors = cv.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  1,  1,  1,  1,  1,  3,  1,  1,  2,  1,  1,  1,  1,  1,  1,  4,\n",
       "         1,  1,  2,  1,  2,  2,  1,  1,  1,  1,  1],\n",
       "       [ 8,  1,  1,  2,  1,  2,  3,  1,  1,  3,  2,  1,  1,  1,  1,  2,  3,\n",
       "         1,  1,  1,  1,  2,  2,  0,  1,  1,  1,  1],\n",
       "       [ 8,  1,  2,  2,  0,  2,  6,  1,  2,  2,  1,  0,  0,  3,  0,  3,  4,\n",
       "         2,  0,  1,  2,  2,  0,  0,  1,  1,  2,  1],\n",
       "       [11,  1,  5,  2,  3,  1,  3,  1,  1,  4,  3,  0,  0,  1,  0,  1,  4,\n",
       "         0,  0,  2,  5,  9,  2,  0,  1,  1,  1,  1]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '.': 1,\n",
       " 'a': 2,\n",
       " 'b': 3,\n",
       " 'c': 4,\n",
       " 'd': 5,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'g': 8,\n",
       " 'h': 9,\n",
       " 'i': 10,\n",
       " 'j': 11,\n",
       " 'k': 12,\n",
       " 'l': 13,\n",
       " 'm': 14,\n",
       " 'n': 15,\n",
       " 'o': 16,\n",
       " 'p': 17,\n",
       " 'q': 18,\n",
       " 'r': 19,\n",
       " 's': 20,\n",
       " 't': 21,\n",
       " 'u': 22,\n",
       " 'v': 23,\n",
       " 'w': 24,\n",
       " 'x': 25,\n",
       " 'y': 26,\n",
       " 'z': 27}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from ``cv.vocabulary_`` that the resulting vectors contain counts of characters, including the spaces and periods.\n",
    "\n",
    "# Character n-grams AND Word n-grams\n",
    "\n",
    "The Burger paper on gender in tweets used character n-grams and word n-grams together. They considered all character n-grams from 1 to 5, and all word 1-grams and 2-grams. Furthermore, they use presence vectors, not count vectors. Using our synthetic data, we'll represent them in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_vectorizer = CountVectorizer(tokenizer=tt.tokenize, binary=True, ngram_range=(1, 2))\n",
    "character_vectorizer = CountVectorizer(tokenizer=list, binary=True, ngram_range=(1, 5))\n",
    "\n",
    "word_vectors = word_vectorizer.fit_transform(texts)\n",
    "character_vectors = character_vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a matrix countaining word counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 49)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 rows, 1 for each text, and 49 columns, so there are 49 unique 1-grams and 2-grams.\n",
    "\n",
    "We also have a matrix countaining character counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 481)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has 481 columns, so in this dataset there are 481 unique character 1-grams, 2-grams, 3-grams, 4-grams, and 5-grams. \n",
    "\n",
    "Using a ``scipy`` function called ``hstack``(standing for ``hstack``), we can glue the rows together, so that we'll have a single new matrix with 4 rows and 481+49=530 columns.\n",
    "\n",
    "``hstack`` takes a list as its argument; in the list, list the matrices you want to paste together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 530)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "vectors = hstack((word_vectors, character_vectors))\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, we have two seaprate vocabulary dictioanries, one for the word n-grams, one for the character n-grams. \n",
    "\n",
    "When we merged the matrices, we **attached the character vectors to the word vectors**. This means the word vector dictionary is correct (their positions didn't change), but we need to update the character n-grams dictionary.\n",
    "\n",
    "We need to change the character n-grams dictionary. Right now, they tell us which character corresponds to the 0th element of a vector, which character corresponds to the 1st element of a vector, and so on. **But since we concatenated the vectors, these positions no longer hold**.\n",
    "\n",
    "If we think about this, solving this will be simple. Right now, the character vocabulary associates the space symbol with index 0. But now, the space symbol occurs *after* all the word n-grams. We know there are a total of 49 word n-grams, so the new index of the space should be 0 + 49 = 49. The character vocabulary associates the bigram \" a\" with index 1. However, since the character n-grams comes after the 49 word n-grams, it should now be associted with the index 1 + 49 = 50.\n",
    "\n",
    "In other words, we should go into the character dictionary and **add 49 to all of the indices**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_ngrams_count = len(word_vectorizer.vocabulary_)\n",
    "\n",
    "new_character_vocabulary = {}\n",
    "\n",
    "for character_ngram, index in character_vectorizer.vocabulary_.items():\n",
    "    new_character_vocabulary[character_ngram] = index + word_ngrams_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're working with the final matrix, and you need to know which words correspond to which index, you can use ``word_vectorizer.vocabulary_`` for the word n-grams and ``new_character_vocabulary`` for the character n-grams. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
