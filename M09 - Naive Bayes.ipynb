{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers are a commona and powerful statistical learning algorithm that is applied in many situations. You may have seen NB classifiers used in some of the papers assigned. To learn about how Naive Bayes classifiers work, we need to discuss the difference between [discriminative](https://en.wikipedia.org/wiki/Discriminative_model) and [generative](https://en.wikipedia.org/wiki/Generative_model) model.\n",
    "\n",
    "For both models, we are looking at a set of data that has values for certain features, which we'll call $X$, and a label that is of interest to us: we'll call the label $Y$. In an SMM scenario, the features $X$ may be the text of teh tweet, and the label $Y$ may refer to the source of tweet: iPhone or Android.\n",
    "\n",
    "The basic idea behind a discriminative model is that we look for patterns of differences in the features $X$ and see if there is a way those differences can help us predict or determine $Y$. In other words, using what we learn about $X$, we can **discriminate** between different $Y$ labels. The mathematical way to state this is that discriminative models are trying to estimate:\n",
    "\n",
    "$$\n",
    "P(Y|X)\n",
    "$$\n",
    "\n",
    "Here, the $P$ means we're discussing a probability. The thing to the *left* of the bar indicates the probability we're trying to estimate, and the thing to the *right* of the bar represents **the information we have to help us make the estimation**. When reading $P(Y|X)$ out loud in natural English language, we say \"the probability of Y given X.\" If we have $X$, the featuers, can we estimate the probability of $Y$? If we know the tweet of the text, can we make a statistically informed guess as to what source it came from? All of the models we have learned up to this point have been discriminative. \n",
    "\n",
    "Generative models take a wholly different approach. They assume that the observations we have of **both** the features **and** the label are the result of some *generative process*. There is a certain probability of a tweet being from Android or iPhone, and there is a certain probability it will have a certain text. The chances of seeing a certain tweet with both a certain text and a certain label have some sort of probability. Generative models therefore try to explore how both the features *and* the labels come about. The mathematical way to state this is that generative models are trying to estimate:\n",
    "\n",
    "$$\n",
    "P(X, Y)\n",
    "$$\n",
    "\n",
    "This, read out loud, is \"the probability of X **and** Y\". For those of you have taken statistics, we refer to this as a **joint probability** as it deals with multiple variables. So if we have a tweet with a certain text, and it's from iPhone, a generative model would try to answer the question, \"What is the probability of getting this text AND it being from the iPhone?\"\n",
    "\n",
    "# From Joint to Conditional\n",
    "\n",
    "Statisticians (in particular, a famous and prolific mathematician named Kolmogrov) have discovered that this relationship is true:\n",
    "\n",
    "$$\n",
    "P(X, Y) = P(X|Y) \\cdot P(Y)\n",
    "$$\n",
    "\n",
    "Let's try to explore what exactly this means by making a nice, easy to understand dataset. \n",
    "\n",
    "| Fur Type | Species|\n",
    "|---|---|\n",
    "|Long|Cat|\n",
    "|Long|Cat|\n",
    "|Long|Dog|\n",
    "|Long|Cat|\n",
    "|Short|Cat|\n",
    "|Short|Cat|\n",
    "|Long|Cat|\n",
    "|Short|Dog|\n",
    "|Short|Cat|\n",
    "|Short|Dog|\n",
    "|Short|Dog|\n",
    "|Short|Dog|\n",
    "\n",
    "And let's say our observed feature, $X$, is the fur type, and the label $Y$, is the species.\n",
    "\n",
    "We can ask the question, what is the probability of a short-haired cat?\n",
    "\n",
    "$$\n",
    "P(\\text{short}, \\text{cat}) = ?\n",
    "$$\n",
    "\n",
    "We simply count the number of instances - 3 - and divide by the number of total observations - 12.\n",
    "\n",
    "$$\n",
    "P(\\text{short}, \\text{cat}) = 0.25\n",
    "$$\n",
    "\n",
    "Now let's take another look at the conditional probability equation above. According to it:\n",
    "\n",
    "$$\n",
    "P(\\text{short}, \\text{cat}) = P(\\text{short} | \\text{cat}) \\cdot P(\\text{cat})\n",
    "$$\n",
    "\n",
    "The probability $P(\\text{short}|\\text{cat})$ is the probability that fur is short *given that we know the animal is a cat*. There are 7 cats, and  3 of them have short hair, so this probability is $3/7 = 0.43$. The probability $P(\\text{cat})$ is simply the probability of any observation being a cat, and there are 7 cats out of 12 observations: $P(\\text{cat}) = 7/12 = 0.58$. Observe that:\n",
    "\n",
    "$$\n",
    "P(\\text{short} | \\text{cat}) \\cdot P(\\text{cat}) = \\frac{3}{7} \\cdot \\frac{7}{12} = \\frac{21}{84} = \\frac{1}{4} = 0.25 = P(\\text{short}, \\text{cat})\n",
    "$$\n",
    "\n",
    "So the above equation is, in fact, true.\n",
    "\n",
    "# Naive Bayes\n",
    "\n",
    "A Naive Bayes classifier works by calculating the joint probability of both the features and the labels in this fashion. It calculates the probabilities of the label class $P(Y)$ *and* the probabilities of features given a label class $P(X|Y)$, which, as we have seen above, is the mathematical equivalent of modeling the joint probability $P(X, Y)$. In the case of the furry cats and dogs data, a Naive Bayes classifier would come up with something like this:\n",
    "\n",
    "$$\n",
    "P(dog) = \\frac{5}{12} \\\\\n",
    "P(cat) = \\frac{7}{12} \\\\\n",
    "P(long|cat) = \\frac{4}{7} \\\\\n",
    "P(short|cat) = \\frac{3}{7} \\\\\n",
    "P(long|dog) = \\frac{1}{5} \\\\\n",
    "P(short|dog) = \\frac{4}{5}\n",
    "$$\n",
    "\n",
    "Now we have to remind ourselves what the original goal for training a classifier is: we have a $new$ set of observations $X$, and we need to predict $Y$. In other words, *if we already know $X$, what is the probability of $Y$?* This sort of brings us full circle; we need to somehow calculate:\n",
    "\n",
    "$$\n",
    "P(Y|X)\n",
    "$$\n",
    "\n",
    "which, if you'll recall, is the task that **discriminative** classifiers have to do! Putting strange observation aside for a moment, the problem we're facing to get the model we created above to *producing predictions* involves invoking something known as Bayes' theorem (and this is why the term Bayes is in Naive Bayes), which takes this form:\n",
    "\n",
    "$$\n",
    "P(Y|X) = \\frac{P(X|Y)\\cdot P(Y)}{P(X)}\n",
    "$$\n",
    "\n",
    "Notice that the **numerator on the right side** can be computed entirely by things that were calculated \"along the way\" by the Naive Bayes model. \n",
    "\n",
    "Through a bit of mathematical trickery, the Naive Bayes model actually *discards the denominator on the right side*. Again, I encourage you to look this up or maybe learn about it in another class that specifically focuses on the mathematics of statistical learning, but 1) since the label we're trying to predict, $Y$, *does not exist in the denominator* and 2) since according to the left side we are assuming that *$X$ is given*, we can ignore $P(X)$. This leaves us with:\n",
    "\n",
    "$$\n",
    "P(Y|X) \\propto P(X|Y)\\cdot P(Y)\n",
    "$$\n",
    "\n",
    "Since we have all the pieces on the right side, then we calculate the value of interest on the left-side. Let's return to our fuzzy animals data.\n",
    "\n",
    "## An Unknown, Long-Haired Animal\n",
    "\n",
    "Let's say we have a new animal come in and it has **long hair**. We want to use our Naive Bayes model to determine the probability that it is a **cat** or a **dog**. The hair length is our $X$, and the species is our $Y$.\n",
    "\n",
    "What's the probability that this long-haired animal is a cat?\n",
    "\n",
    "$$\n",
    "P(\\text{cat}|\\text{long}) \\propto P(\\text{long}|\\text{cat}) \\cdot P(\\text{cat}) = \\frac{4}{7} \\cdot \\frac{7}{12} = 0.33\n",
    "$$\n",
    "\n",
    "How about a dog?\n",
    "\n",
    "$$\n",
    "P(\\text{dog}|\\text{long}) \\propto P(\\text{long}|\\text{dog}) \\cdot P(\\text{dog}) = \\frac{1}{5} \\cdot \\frac{5}{12} = 0.08\n",
    "$$\n",
    "\n",
    "Since $0.33 > 0.08$, the classifier would predict the label \"cat\" for this new, long-haired animal.\n",
    "\n",
    "## An Unknown, Short-Haired Animal\n",
    "\n",
    "We take a similar approach. What's the probability that this is a cat?\n",
    "\n",
    "$$\n",
    "P(\\text{cat}|\\text{short}) \\propto P(\\text{short}|\\text{cat}) \\cdot P(\\text{cat}) = \\frac{3}{7} \\cdot \\frac{7}{12} = 0.25\n",
    "$$\n",
    "\n",
    "as for a dog:\n",
    "\n",
    "$$\n",
    "P(\\text{dog}|\\text{short}) \\propto P(\\text{short}|\\text{dog}) \\cdot P(\\text{dog}) = \\frac{4}{5} \\cdot \\frac{5}{12} = 0.33\n",
    "$$\n",
    "\n",
    "In this case, since $0.25 < 0.33$, the Naive Bayes classifier will classify the short-haired unknown as a dog.\n",
    "\n",
    "# Multiple Features\n",
    "\n",
    "As has been the case previously, I've introduced this statistical learning approach using only a single feature (fur length) and a binary class (cat or dog). When we have only one feature and two labels, you really don't need a fancy Naive Bayes classifier. In fact, looking at the data table above, you could simply employ human intuition: \"Well, cats tend to be long-haired, and dogs tend to be short-haired, so if we see something long-haired, it's probably a cat, and if it's short-haired, it's probably a dog.\" This kind of \"intuition\" is *precisely* what a Naive Bayes classifier is doing, in mathematical terms, when there is one feature and a binary label.\n",
    "\n",
    "Naive Bayes classifiers become much more useful, of course, when we're dealing with many features, and human intuition is unable to keep up. In this scenario, if we have $n$ we can say that the feature variable $X$ is actually a vector of values $\\{X_1, X_2, ..., X_n\\}$. In this case, we can re-express the Naive Bayes approach as:\n",
    "\n",
    "$$\n",
    "P(Y|X_1, X_2, ..., X_n) \\propto P(X_1, X_2, ..., X_n|Y)\\cdot P(Y)\n",
    "$$\n",
    "\n",
    "How do we calculate $P(X_1, X_2, ..., X_n|Y)$? Here, the Naive Bayes approach makes a very large assumptions.\n",
    "\n",
    "Above, I introduced the equation for conditional probability:\n",
    "\n",
    "$$\n",
    "P(X, Y) = P(X|Y) \\cdot P(Y)\n",
    "$$\n",
    "\n",
    "Note that this holds ONLY if the events $X$ and $Y$ are dependent on each other: if they're somehow connected, and the outcome of one relies on the outcome of the other. If this is not the case, then two events are said to be **independent**, and when two events are independent, their joint probability can be calculated as follows:\n",
    "\n",
    "$$\n",
    "P(X, Y) = P(X) * P(Y)\n",
    "$$\n",
    "\n",
    "This is *only* the case when the events are independent. Imagine you were rolling a die and flipping a coin. The outcome of one has not effect on the other. They are *independent*. The probability of getting a 4 on the die and heads on the coin is:\n",
    "\n",
    "$$\n",
    "P(\\text{die six}, \\text{heads}) = P(\\text{die six}) \\cdot P(\\text{heads}) = \\frac{1}{6} \\cdot \\frac{1}{2} = 0.083\n",
    "$$\n",
    "\n",
    "There's a one in twelve chance you'll get a six on the die and heads on the coin.\n",
    "\n",
    "Back to Naive Bayes. An NB classifier **makes the assumption that the feature variables $X$ are independent**. Therefore, we can make the following transormation:\n",
    "\n",
    "$$\n",
    "P(X_1, X_2, ..., X_n|Y) = P(X_1|Y) \\cdot P(X_2|Y)\\cdot ... \\cdot P(X_n|Y)\n",
    "$$\n",
    "\n",
    "Each of the $X_n$ features are assumed to be independent, therefore the conditional probability $P(X_n|Y)$ is easily to calculate: simply view it in isolation, and calculate the probability while completely ignoring the existence of other features.\n",
    "\n",
    "# How to Calculate $P(X_n|Y)$\n",
    "\n",
    "## Feature is Categorical\n",
    "\n",
    "When the feature is discrete, calculating $P(X_n|Y)$ is simply. Just use a proportion. Here's the animal data again:\n",
    "\n",
    "| Fur Type | Species|\n",
    "|---|---|\n",
    "|Long|Cat|\n",
    "|Long|Cat|\n",
    "|Long|Dog|\n",
    "|Long|Cat|\n",
    "|Short|Cat|\n",
    "|Short|Cat|\n",
    "|Long|Cat|\n",
    "|Short|Dog|\n",
    "|Short|Cat|\n",
    "|Short|Dog|\n",
    "|Short|Dog|\n",
    "|Short|Dog|\n",
    "\n",
    "To calculate $P(\\text{short}|\\text{dog})$, simply county how many dogs have short hair, and divide it by the number of dogs. \n",
    "\n",
    "$$\n",
    "P(\\text{short}|\\text{dog}) = \\frac{4}{5}\n",
    "$$\n",
    "\n",
    "In scikit-learn, we usually transform categorical labels into one-hot encoding representations, which was covered in the lecture/notebook on representing texts as numbers.\n",
    "\n",
    "## Feature is a Discrete Count\n",
    "\n",
    "Let's say that your feature is an integer-valued count of something. This is the case when you use the vector space model to represent texts.\n",
    "\n",
    "|Count \"economy\"|Count \"terrorism\"|Source Candidate|\n",
    "|---|---|---|\n",
    "|1|2|Trump|\n",
    "|0|1|Trump|\n",
    "|1|1|Trump|\n",
    "|2|1|Clinton|\n",
    "|1|1|Clinton|\n",
    "\n",
    "We usually calculate $P(X_1, X_2|Y)$ using something called a [multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution), which conceptualizes each feature as a set of independent binomial distributions.\n",
    "\n",
    "## Feature is a Continuous Variable\n",
    "\n",
    "Finally, a feature may be a continuous variable rather than a binary or integer variable, as with the classic Iris data. In this instance, an NB classifier must assume that the values seen for a given feature are drawn from some sort of random distribution. Often, we assume that they are drawn from a normal, or Gaussian distribution: the bell curve. \n",
    "\n",
    "Take the case of sepal length and iris species. How would we calculate $P(\\text{sepal length}=4.3 | \\text{Iris Virginica})$? What a Gaussian Naive Bayes classifier would do is look at the training data and collect the sepal lengths of all the Iris Virginica samples. It will assume that there is a bell curve distribution that has the same mean as the sample mean, and the same standard distribution as the sample standard distribution. Using *this* bell curve, the classifier then classifies the probability of a sepal length of 4.3 cm. \n",
    "\n",
    "# Naive Bayes with Scikit-Learn\n",
    "\n",
    "Scikit-Learn has 3 Naive Bayes factor methods:\n",
    "\n",
    "1. GaussianNB\n",
    "2. MultinomialNB\n",
    "3. BernoulliNB\n",
    "\n",
    "Use **GaussianNB** if your features are continuous.\n",
    "\n",
    "Use **MultinomialNB** if your features are counts. This the case of count vectorized text.\n",
    "\n",
    "Use **BernoulliNB** if your features are binary. This is the case with one-hot encoded categories, *and* texts that are converted into presence vectors (rather than counting the number of times a word occurs, simply use a 1 if the word occurred and a 0 if it didn't).\n",
    "\n",
    "## GaussianNB\n",
    "\n",
    "We'll use the Iris data to demonstrate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy, 6-fold cross validation:\n",
      "0.952932098765\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(gnb, X, y, scoring=\"accuracy\", cv=6)\n",
    "\n",
    "import numpy as np\n",
    "print(\"Average accuracy, 6-fold cross validation:\")\n",
    "print(np.mean(scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the GaussianNB does extremely well with the Iris data.\n",
    "\n",
    "## MultinomialNB\n",
    "\n",
    "I'll import the Trump and Clinton tweets and then represent them as count vectors. Then we'll train a MultinomialNB classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>fav</th>\n",
       "      <th>rt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Remember, don't believe \"sources said\" by the ...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>8572</td>\n",
       "      <td>3616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Did Crooked Hillary help disgusting (check out...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>12930</td>\n",
       "      <td>5628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Using Alicia M in the debate as a paragon of v...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>12738</td>\n",
       "      <td>5209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Wow, Crooked Hillary was duped and used by my ...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>13210</td>\n",
       "      <td>5205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Anytime you see a story about me or my campaig...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>21316</td>\n",
       "      <td>9147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              user                                               text  \\\n",
       "0  realDonaldTrump  Remember, don't believe \"sources said\" by the ...   \n",
       "1  realDonaldTrump  Did Crooked Hillary help disgusting (check out...   \n",
       "2  realDonaldTrump  Using Alicia M in the debate as a paragon of v...   \n",
       "3  realDonaldTrump  Wow, Crooked Hillary was duped and used by my ...   \n",
       "4  realDonaldTrump  Anytime you see a story about me or my campaig...   \n",
       "\n",
       "                source    fav    rt  \n",
       "0  Twitter for Android   8572  3616  \n",
       "1  Twitter for Android  12930  5628  \n",
       "2  Twitter for Android  12738  5209  \n",
       "3  Twitter for Android  13210  5205  \n",
       "4  Twitter for Android  21316  9147  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"sentimentanalysis.csv\", index_col=0, encoding=\"latin1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10767)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(df['text'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy, 10-fold cross validation:\n",
      "0.9134\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "y = df['user']\n",
    "\n",
    "scores = cross_val_score(mnb, X, y, scoring=\"accuracy\", cv=10)\n",
    "print(\"Average accuracy, 10-fold cross validation:\")\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BernoulliNB\n",
    "\n",
    "To demonstrate ``BernoilliNB``, I'll simply *binarize* the count vectors I made of the Clinton/Trump tweets. Instead of a number representing the count, we'll indicate if the word is present with a 1, and absent with a 0.\n",
    "\n",
    "Note that to do this, we simply take the count vectors and replace **every number that is greater than 0 with a 1** and keep all the 0s as they are. \n",
    "\n",
    "As always, there's a great sklearn function that helps us do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "\n",
    "X = binarize(X)\n",
    "# Now, instead of each element representing a count of a word, there is a 1 to indicate presence and 0 to indicate absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy, 10-fold cross validation:\n",
      "0.917\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "y = df['user']\n",
    "\n",
    "scores = cross_val_score(bnb, X, y, scoring=\"accuracy\", cv=10)\n",
    "print(\"Average accuracy, 10-fold cross validation:\")\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the binarization of our count vectors gets us a *slight* boost - probably not statistically significant. For your projects, I'd recommend seeing both how count vectors and binarized count vectors work for your data."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
