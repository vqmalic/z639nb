{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# How not to find model hyperparameters\n",
    "\n",
    "In my presentation on visualization, I did something that was very much a bad thing to do in machine learning. I'm curious to know if any of you caught it!\n",
    "\n",
    "Specifically, this occurred when I showed you how you could use visualizations to help understand what is happening with the performance of a model.\n",
    "\n",
    "I took the Iris dataset and split it into train and test sets. I wanted to use a kNN classifier to predict Iris species, but I wasn't sure which value of ``k`` would be the best one. \n",
    "\n",
    "To find the best value of ``k``, I made many models with many different values of ``k``. I recorded each model's accuracy on the test set, and then visualized them. \n",
    "\n",
    "I then found the ``k`` value with the highest accuracy. We could then train a model with that value of ``k`` and use that as our \"production\" model to predict the species of any new measurements of Iris plants we take in the future.\n",
    "\n",
    "# The Problem: Part 1\n",
    "\n",
    "You'll recall from a few lectures back that the major goal in statistical learning is to build a model that **generalizes well**, a model that performs well on data that it is not trained on and has not been overfit or too precisely calibrated to the training data. This is the *exact* reason why we produce a train-test split in the first place. This division is sarcosanct: it *must not* be violated. \n",
    "\n",
    "The problem with the approach I took in the visualization lecture is that by choosing the value for the hyperparameter ``k`` on the performance of the model on the test set, I was breaking the wall we must maintain between training and testing data. Choosing ``k`` based on the test data is a way of *indirectly training the model on the test set*. While we are not *directly* training any of the models on the test data, we are allowing the test data to influence a setting of the model, namely, the number of nearest neighbors. This breaks a cardinal rule: the test set should *never* have any influence at all on the model, even when it comes to choosing hyperparameter settings. It should *only* be used in the final stages of your research, as a way to evaluate the model you've made. If you put in tons and tons of work and then get horrible performance on the test data, that's the end of the story: the model probably doesn't work on your data. If you go back and then try to tweak your model so it gets a better score on the test set, it's sort of a form of machine learning \"cheating.\"\n",
    "\n",
    "What to do then? Just guessing the hyperparameter and seeing what you get on the test set seems like a risky proposition. At the same time, we can't choose a hyperparameter based on performance on the training set, becuase that will likely lead to overfitting on the *training set*. \n",
    "\n",
    "# The Solution: Part 1\n",
    "\n",
    "I'm sure many of you will immediately think up a very intuitive solution to this conundrum. Make *another split*. Specifically:\n",
    "\n",
    "1. Split your data into training and testing data. Set the testing data aside, don't look at it until you're ready for a final evaluation. \n",
    "2. Split your *training* data into two sets. Train on one, evaluate on the other, and tweak your hyperparameters accordingly. \n",
    "\n",
    "This is exactly what researchers did to address this problem. Specifically, once you've set aside your test set, you take the remaining data and split it yet again, with one set being called the training set, and the other being called [the validation set](https://en.wikipedia.org/wiki/Test_set#Validation_set). Without even peeking at the test set, you train on the training set and then find the best hyperparameters according to performance on the validation set. We now therefore have 3 sets. Frequently in the statistical learning community researchers go with a 60/20/20 split: 60% training, 20% validatation, and 20% test. \n",
    "\n",
    "# The Problem: Part 2\n",
    "\n",
    "But then the same problem occurs when we do just that. We train on the train set, and evaluate on the validation set, then pick the hyperparameter (like the value for ``k``) based on which value attained the heighest accuracy on the validation set. \n",
    "\n",
    "But won't this mean the model will overfit on the *validation* set? Just like earlier we risked overfitting on the test set?\n",
    "\n",
    "The problem remains. As always, the scientific community has come up with a solution: it's called k-fold cross validation.\n",
    "\n",
    "# The Solution: Part 2, or  K-Fold Cross Validation\n",
    "\n",
    "The basic idea behind k-fold cross validation is that you don't just make one validation set, you make many. Here's how it works:\n",
    "\n",
    "1. Randomly split your data, designating one piece as testing data. Set that aside. \n",
    "2. Shuffle the remaining (non-testing) data. \n",
    "3. Divide it into k equally-sized sets. Let's say in our case k is 10. We call these equally sized sets **folds**. \n",
    "4. Let the first 9 folds be the training set, and the 10th be the validation set. Train on the first 9 folds and get an evaluation metric like accuracy on the 10th. \n",
    "5. Then, let the 9th fold be the validation set. Train on folds 1-8 and 10, evaluate on the 9th. \n",
    "6. Then, let the 8th fold be the validation set. Train on sets 1-7 and 9-10, evaluate on the 8th. \n",
    "7. And so on, until you run out of folds. \n",
    "8. When your done, you have 10 accuracy values, one for each fold. Calculate the average of those accuracies and you have an idea of how your model performs on 10 different validation sets. \n",
    "\n",
    "Here is a visualization of the 10-fold cross validation that will hopefully make things clearer:\n",
    "\n",
    "<img src=\"http://ella.ils.indiana.edu/~vmalic/z639/10_fold_cv.png\">\n",
    "\n",
    "How does this work? Well, we're worried that if we had one validation set, we might overfit to the validation set. By random chance, given how the validation set was split off, we might find a hyperparameter value that works *well for that one validation set* but does not generalize well.\n",
    "\n",
    "But by performing k-fold cross validation, we get multiple accuracies. It might accidentally work well for one of the validation sets, but that gets averaged out when we consider the accuracy for the other k-1 datasets. \n",
    "\n",
    "# Choosing K (as in fold, not NN)\n",
    "\n",
    "How do you choose how many folds you should have? As always, this depends on the data. If you choose 1000-fold CV, then each of the validation sets will only have a small amount of data, so the accuracies may vary wildly. If you choose 2-fold CV, there is a lot of data in the validation set, but you only get two accuracies out of it. You should mostly consider how many samples you have. If you have a million samples, a 1000-fold CV is possible, since each validation set would have 1000 samples in it - but then you'd have to take into account how long it would take to train 1000 models. When it comes to what you see most often in the literature, 10-fold CV seems to be quite common. \n",
    "\n",
    "# Doing this in Scikit-Learn\n",
    "\n",
    "Doing k-fold cross validation from scratch is a very tedious task, with many steps. Once again, scikit-learn saves the day and gives us a simple method that basically does it all for us. \n",
    "\n",
    "Let's get the Iris data and train a k-NN classifier on it. *For now*, I'll use ``k=5``. \n",
    "\n",
    "First, we split the data into train and test. I'll make the test size .33, since there are 150 samples. That will give us a testing set size of 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n",
      "(50, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(3057)\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1234)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize our Nearest Neighbor classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we import a method from sklearn called ``cross_val_score`` that will do all the cross validation steps for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 parameters we must pass to the method ``cross_val_score``. \n",
    "\n",
    "1. The untrained model. In our case, this is the variable ``clf``.\n",
    "2. The training data, ``X_train``. ``cross_val_score`` will generate the validation sets automatically.\n",
    "3. The training labels, ``y_train``. \n",
    "4. A parameter called ``scoring`` which takes a string as its value. This indicates *what kind of metric* you want to use. The string `accuracy` uses accuracy. The string ``r2`` will calculate R-squared for regression problems. A full list of strings for metrics you can use can be [found here](http://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values). \n",
    "5. A parameter called ``cv`` indicating the number of folds. \n",
    "\n",
    "``cross_val_score`` returns a list that is the same length as the parameter ``cv``. It lists the evaluation metric obtained for each fold.\n",
    "\n",
    "I'm going to use ``cross_val_score`` to run 10-fold cross validation on the training data. I'm going to use accuracy as teh scoring metric (but I could use F1 scores or recall or whatever if I wanted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          1.          1.          0.8         0.88888889  0.88888889\n",
      "  1.          0.88888889  1.          1.        ]\n"
     ]
    }
   ],
   "source": [
    "cv_scores = cross_val_score(clf, X_train, y_train, scoring=\"accuracy\", cv=10)\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, for the 1st, 2nd, and 3rd folds our 5NN classifier got 100% accuracy on the held-out validation set. It got 80% on the 4th, 89% on the 5th, and so on. 80% is pretty bad compared to the others: imagine if we had only one validation set and by complete chance it was this same one that got us an 80% accuracy. We'd conclude ``k=5`` is bad, without knowing that ``k=5`` would have 100% accuracy on a completely different validation set. This is why we make many validation sets.\n",
    "\n",
    "By averaging these accuracies we can get a composite score that gives us an idea how well our classifier did over all folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.946666666667\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95%: some did well, some didn't do as well. But overall, we got a pretty high score for ``k=5``. \n",
    "\n",
    "# Using CV for Hyperparameter Optimization\n",
    "\n",
    "Let's go back to the original problem: how do we find the optimal value for ``k``?\n",
    "\n",
    "What we do is we first select a set of candidate values for ``k``. Like in the visualization demonstration, I'm going to use every odd number from 1 to 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_candidates = np.arange(1, 50, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For *each hyperparameter candidate*, run k-fold cross validation. Get and save the average accuracy attained during each CV. I'll do 10 fold for this example. \n",
    "\n",
    "Then, see which hyperparameter had the highest average accuracy. **That's** your optimal value for k. \n",
    "\n",
    "Finally, using only that value of ``k``, evaluate on the held-out test data. This is the final accuracy you report to your boss or in your research paper. \n",
    "\n",
    "Let's do it in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "average_accuracies = []\n",
    "\n",
    "for k in k_candidates:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    cv_scores = cross_val_score(clf, X_train, y_train, scoring=\"accuracy\", cv=10)\n",
    "    average_accuracy = np.mean(cv_scores)\n",
    "    average_accuracies.append(average_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the average accuracies for value of k using visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEPCAYAAABRHfM8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xm4VXXZ//H3BwFnQcQhQUBBVJyAo4iadgBLHHIgK+jJ\ngQit1HietJ9mmce0QsvM0jSHHIrEIQecgfTgUAjCEQGZHEBwQFRUQEWG+/fHd23YHs6w9j577fF+\nXde+3HvtNdxnXXjus77D/ZWZ4ZxzzjWnVaEDcM45Vxo8YTjnnIvFE4ZzzrlYPGE455yLxROGc865\nWDxhOOeciyXxhCFpsKS5kuZLuqCB79tLuk/SDEmTJfVK+66dpHskzZE0W9IhScfrnHOuYUpyHoak\nVsB8YBDwFjAVGGpmc9P2uRJYYWaXSdoLuM7Mjoq+uw2YZGa3SmoNbGVmHycWsHPOuUYl/YTRD1hg\nZovMbA0wFjix3j69gCcBzGwe0E3SjpK2A44ws1uj79Z6snDOucJJOmF0AhanfV4SbUs3AxgCIKkf\n0AXoDOwOvCfpVknTJd0oacuE43XOOdeIYuj0Hg1sL2k6cDZQB6wDWgN9CU1UfYFPgAsLFqVzzlW4\n1gmf/03CE0NK52jbBma2Avhe6rOk14HXgK2BxWb2QvTVvcAmnebRMV4QyznnMmRmymT/pJ8wpgI9\nJHWV1BYYCoxL3yEaCdUmej+S0Mm90syWAosl9Yx2HQS83NiFzMxfZlxyySUFu/akSUaPHsaFF1b2\nfSi2l98LvxcNvbKR6BOGma2TdA4wnpCcbjGzOZLOCl/bjcA+wO2S1gOzgRFpp/gxMCZKKK8Bw5OM\n12Vn1Sr42c/g3nth2DCYPLnQETnnkpB0kxRm9jiwV71tf017P7n+92nfzQAOTjRA1yK1tTBiBBx+\nOMyaBevXQ/fu4b+tiqGHzDmXM4knDJdf1dXVebnOihVwwQXw0ENwww1w3HEbv2vfHl59FfbcMy+h\nNChf96EU+L3YyO9FyyQ6cS9fJFk5/BylYuJEGDkSBg6Eq64KCSLdkCHwrW/B0KGFic851zxJWIad\n3v6EUSAffQQnnQTLl2d+7F57wVFHhdfuu+c+tsZ8/DGcfz48/jjceCMMHtzwflVVMG2aJwznyo0n\njAK56y7Yaiu4+urMjlu/HmbMCH/lX3wxbL01DBoUksfAgdCxYzLxPvEEnHlmSBKzZsF22zW+b1UV\nXHllMnE45wrHm6QK5NBD4ec/h+OPz/4cZjB7dkge//43PP106HBOJZAjjghJqSU+/BB+8hN46im4\n6aZw3uYsWxb6L5YvB2X0wOucy5dsmqQ8YRTA3LkwYAAsXgytc/iMt2YNTJkSksfEiTB9Ohx8cEgg\nVVWZX+udd+Cii+CEE2D0aNh22/jHdukCTz4JPXpkdk3nXH54wigRF14YmpaSbrZZuRKeeSYkj5kz\nwxNJJtq2DX0WAwZkfu2TTw59GN/+dubHOueS5wmjBKxdG/76njgRevVqfv9SdfnloZPc+zKcK07Z\nJAyfWpVn48fDbruVd7KAjSOlnHPlwxNGnt16KwyvgAInVVWhD6VEHvycczF4k1Qevf8+7LEHLFq0\n6WS3crTbbqF0SPfuhY7EOVefN0kVuTvvhGOPrYxkAd4s5Vy58YSRR5XSHJWSapZyzpUHTxh58tJL\n8O67YU5Epejb158wnCsnnjDy5Lbb4PTTYbPNCh1J/qSapEqge8k5F4PXksqDNWtgzBh47rlCR5Jf\nu+wCW24JCxfmt0iicy4Z/oSRB488Aj17VmaZDO/4dq58eMLIg0rr7E7nCcO58uEJI2FLl8KkSfDN\nbxY6ksLwhOFc+fCEkbAxY8JCSZlUei0n3vHtXPnwhJEgs9AcdcYZhY6kcL70Jdh88zC73TlX2jxh\nJGjaNFi1Co48stCRFJY3SzlXHjxhJCj1dNGqwu+yJwznykPiv8okDZY0V9J8SRc08H17SfdJmiFp\nsqRead8tjLbXSZqSdKy59NlnMHZsmKxX6TxhOFceEp24J6kVcC0wCHgLmCrpQTObm7bbRUCdmQ2R\ntBdwHZBaOXo9UG1my5OMMwnjxkHv3tC1a6EjKbz0jm9f49u50pX0E0Y/YIGZLTKzNcBY4MR6+/QC\nngQws3lAN0k7Rt8pDzEmopLnXtS3667Qpg288UahI3HOtUTSv4w7AYvTPi+JtqWbAQwBkNQP6AJ0\njr4zYIKkqZJGJhxrzrz5Jjz/PAwZUuhIioc3SzlX+oqhltRo4BpJ04GZQB2wLvrucDN7O3rimCBp\njpk929BJampqNryvrq6muro60aCbcscdcMopsNVWBQuh6KRKnXsSda4wamtrqa2tbdE5El1xT1J/\noMbMBkefLwTMzK5o4pjXgf3NbGW97ZcAK8zsDw0ck7MV9954Azp1yr6qrBnsvXdokjrssJyEVBbG\njYPrr4fHHit0JM45KM4V96YCPSR1ldQWGAqMS99BUjtJbaL3I4FJZrZS0laStom2bw18DZiVZLCf\nfw59+sCXvwxz5mR3jv/+N3TsHnpobmMrdT7j27nSl2jCMLN1wDnAeGA2MNbM5kg6S9KZ0W77ALMk\nzQGOBkZF23cGnpVUB0wGHjKz8UnGO2FCeDo47TQ44ggYPRrWrs3sHKm5Fz4a6It23TXckyVLCh2J\ncy5biTZJ5UuumqROPRX694ezzw5rOIwcCR9+GJLAfvs1f/yqVdC5M8yeHX5Bui869lg488xQW8s5\nV1jF2CRVMj79FB5+OHRWA3TrBuPHh19wAwbA5ZeHhZCacv/9oSnKk0XDfKSUc6XNE0bk0UfhoINg\n5503bpPCU8a0afDss3DIITBjRuPnqPRCg83xhOFcafOEERk7FoYObfi7Ll3C6J5zz4WvfhVqakIH\nebqFC0MyOeGEpCMtXd7x7Vxp84QBrFgRmp9OPrnxfaQwc7uuDl54AQ4+OMwrSLn99pBwttgi+XhL\nVefOIVm8+WahI3HOZaPZhCHpXEnb5yOYQhk3LpQg79Ch+X07dYKHHoLzz4fBg+EXvwiFBm+/3UuB\nNEfyZinnSlmcmd47E4oGTgf+BjyRs1lyRWLsWPj2t+PvL4URVUcdBT/8IfTsCe3aQd++ycVYLlIJ\n48T6FcWyMH16eOLLh223DQMiKr1UvatssYbVShJh4txw4CDgbuAWM3s12fDiacmw2g8+gN13h8WL\nYbvtMj/eDO65JySMo4/OKoSKcv/9cPPN8MgjLTvPypXQvTscc0z2s/IzMWkS/PrXmf1h4Vwxy2ZY\nbaxaUmZmkt4B3gHWAtsD90qaYGb/L/NQi8f994eO7GySBYSnjW99K7cxlbOqqvBU1tJS59ddB9XV\ncNttuYqsaY89BuedF54y8pGgnCtGcfowRkmaBlwJPEeo8/RDoAr4RsLxJa6p0VEu93bbDdatg7fe\nyv4cK1bAVVfBJZfkLq7mDB4c/qi4++78XdO5YhOnRbYDMMTMjjaze6J1LTCz9cDxiUaXsKVLYerU\nMAPZ5UcuOr6vvRYGDYJevZrfN1ckuPRS+NWvQsJzrhLFSRiPAR+kPkjaTtIhAGaWZYm+4nDvvXD8\n8V6GPN9Spc6z8fHHcPXV+X26SPna18JIurFj839t54pBnIRxPZBeanxltK3keXNUYbTkCePPfw6/\nuPfeO7cxxZH+lJFpUUrnykGchPGFIUhRU1QxLLzUIosXw8svh18+Lr+yTRgffQR//CP88pe5jymu\nQYNgp53gzjsLF4NzhRInYbwm6ceS2kSvUcBrSQeWtLvvDjO727YtdCSVp0uXUFrl7bczO+5PfwrD\naHv2TCauOPwpw1WyOAnjB8BhwJuENbkPAc5s8ogS4M1RhZNNx/eHH4aEcfHFycUV14ABoSLxmDGF\njsS5/Go2YZjZu2Y21Mx2MrOdzew7ZvZuPoJLyiuvhKVYC7jsd8Xr2zezhHHNNXDccbDnnsnFFFfq\nKeOyy/wpw1WWZvsiJG0BjAD2BTaU1jOz7yUYV6Luugu++U1oXfI9MaWrqgruuCPevh9+GDq7n38+\n2ZgyUV0d5pT8/e9eQ8xVjjhNUn8HdiEsnzoJ6AysSDKopHlzVOFl0iR19dWhbHz37snGlKnUU0Zz\nC2s5Vy6arSUlqc7M+kh6ycwOkNQGeMbM+ucnxOZlUktq1qzQcbpokReSKyQz6NgxLGe7yy6N77d8\neWiGmjIF9tgjf/HFNWgQfOc7MGJEoSNxLjNJLdGa+vvpQ0n7Ae2AnTINrljcdVcoIOfJorCkeP0Y\nf/hDWAO8GJMFhKeMyy/fdEEt58pRnF+bN0brYfwCGAe8DFyRaFQJMfPmqGLSXLPUBx/A9deHNUeK\n1Ze/DD16hPVQnCt3TSYMSa2Aj81suZk9bWZ7RKOl/pqn+HJq+vSQNKqqCh2Jg+YTxlVXwZAh0K1b\n3kLKyqWXhtLn/pThyl2TCSOa1V3S5cvTpZ4uWlJW2+VOUwnjvffghhvg5z/Pb0zZOOww2GsvuPXW\nQkfiXLLiNElNlHS+pN0kdUi94l5A0mBJcyXNl3RBA9+3l3SfpBmSJkvqVe/7VpKmSxoX95oNWb8+\nzO725qjisfvu8MknoWpwfVddFYY+d+2a/7iykXrKWL260JE4l5w4CePbwNnA08C06PVCnJNHTVrX\nEobk7gsMk1S/bNxFQJ2ZHQicDvyp3vejCP0mLTJ5clhmc7/9WnomlyuNdXwvWwY33ggXXVSYuLLR\nvz/suy/87W+FjsS55MSZ6b17A6+4Y1b6AQvMbFG0jsZYoP5qzr2AJ6NrzQO6SdoRQFJn4Fjg5pjX\na5R3dhenhkqd//73YSRbly6FiSlbl14Kv/mNP2W48hVnpvdpDW03szjzdDsBi9M+LyEkkXQzgCHA\nc5L6AV0IkwOXAVcDPyUM5c3aunVh3e2nn27JWVwSqqq+WPn13XfDmt8zZhQupmz16wcHHBDiP/vs\nQkfjXO7FKY5xcNr7LYBBwHQgZmGHZo0GrpE0HZgJ1AHrJB0HLDWzFyVVA012VdfU1Gx4X11dTXVa\noahJk6BTp+KoQ+S+qKoKzj9/4+ff/Q6GDYPOnQsXU0vU1IQqyCNGwBZbNLu7c3lTW1tLbW1ti87R\n7EzvTQ6Q2gNjzWxwjH37AzWpfSVdCJiZNTqPQ9JrwAGEvo3vAmuBLYFtgfvMbJMnnuZmep95ZiiJ\nnf6LyRUHs7CK3fz5YWBCr17w0kshwZeqr389rLNy7rmFjsS5xmUz0zubhNEGmGVme8XYdzNgHuGp\n5G1gCjAsfWlXSe2AT8xsjaSRwOFmdka983wFOM/MTmjkOo0mjM8/D6Wop08vvTbxSjFoEPz0pzBh\nQqjL9Kf6wx5KzLRpofbVK6/AllsWOhrnGpZNwojTh/EQkPpt3IrQSX13nJOb2TpJ5wDjo2NvMbM5\nks4KX9uNwD7A7ZLWA7MJlXFzZuLEsJynJ4viVVUFjzwC//wnzJxZ6GharqoKDjoojPQaNarQ0TiX\nO3GKD34l7eNaYJGZLUk0qgw19YRx2mmhM/Kcc/IclIvtrrtCv8WPfxyWYC0HdXVh/Y5XX/WnDFec\nEmmSkrQ78LaZfRZ93hLY2cwWZhtorjWWMD79NDRHzZnTdEVUV1ivvx5GF82fD1/6UqGjyZ2TT4aB\nA70vwxWnpKrV3gOsT/u8LtpW9B57LDQPeLIobrvvDm+9VV7JAkKiuPnm0LHvXDmIkzBam9mGsmrR\n+7bJhZQ7PlmvdGy7baEjyL3qavj449A85Vw5iDMPY5mkE8xsHICkE4H3kg2r5VasgCeeCAXsnCuE\nVq3g9NPhtttCCRSXGTN47rnQtJyJzTaDI4/0JZiTEKcPozswBtg12rQEOM3MXkk4ttga6sOYNSuM\nUin1IZqutL3+ehh0sWQJbL55oaMpLffeG0aZ9erV/L7pXnoprE8yuNmZYpUt0XkYkrYBMLOVWcSW\nqEyWaHUu3wYOhB/9CE45pdCRlJbq6nDfvvWtzI776U+hXbviXnirGCTS6S3pN5Lam9lKM1spaXtJ\nl2cfpnOV5YwzfK2MTM2cCQsWhJFmmWpuYS6XvTid3seY2YepD2a2nFBB1jkXwze+Af/5TxgJ5uL5\ny19CSZ82bTI/tqEKyC434iSMzSRtaH2N5mF4a6xzMW29dUga//hHoSMpDR99FEY4nnlmdsd37x7O\n8V7RD80pPXESxhjg35JGSBoBTCB3lWqdqwjDh4dmKe9qa97tt8PRR2c/L6dVK+jTx5ulkhBnAaUr\ngMsJNZ/2AS5rqtqsc25Thx0W1mV5/vlCR1Lc1q+H665reSkf78dIRpwnDMzscTM738zOB1ZJui7h\nuJwrK5J3fscxcWJYR+Tww1t2Hk8YyYiVMCT1kXSlpIXAZcDcRKNyrgyddlpY+fGTTwodSfFKPV0o\no8Gem/KEkYxGE4aknpIukTQX+DNhqVWZ2QAz+3PeInSuTHTuHCbxPfBAoSMpTgsXwrPPwne+0/Jz\n9egBy5fD+++3/Fxuo6aeMOYCA4HjzezLUZJYl5+wnCtP3izVuBtuCKVUtt665efyju9kNJUwhhBW\nyXtK0k2SBtHMutrOuaaddFKYI/DGG4WOpLh89hnccgv88Ie5O6c3S+VeownDzB4ws6HA3sBTwP8C\nO0m6XtLX8hWgc+Vkiy3g298OQ0fdRnfdFVYp3HPP3J3TE0buxRlWu8rM/mlmXwc6A3XABYlH5lyZ\nGj48VLD1ORkbXXtt7lfF7NvXE0auxRollWJmy83sRjMblFRAzpW7gw4KTxrPPFPoSIrDlCmhczrX\n1WV79gzn9Y7v3MkoYTjnWk7aOPPbhaeLH/0orGORS61aQe/eXlcqlzxhOFcA3/1uGF67sugWC8iv\nZcvgoYfge99L5vxeiDC3mpqHcZ2kFs63dM41ZJdd4IgjwkS+SnbzzTBkCHTokMz5veM7t5p6wpgP\n/F7SwmiWd598BeVcJTjjjND5XanWrg1zL84+O7lreMLIraaG1V5jZocCXwHeB/4maW40+7tn3AtI\nGhwdN1/SJqOrJLWXdJ+kGZImS+oVbd9c0vOS6iTNlHRJFj+fc0Xr+ONhzhx49dVCR1IYDz8MnTol\nu955z57w7rth1rdruTjDaheZ2RVm1gcYBpwEzIlzckmtgGuBo4F9gWGS9q6320VAnZkdCJwO/Cm6\n7mpgQHTd3sAxkvrF+7GcK35t24YyGJX6lHHttck+XUDoSPeO79yJs0Rra0lflzQGeAyYR5gFHkc/\nYEGUdNYAY4ET6+3TC3gSwMzmAd0k7Rh9TpVp2xxoDfjIdVdWhg8Pk/jWVVjRnTlzYNas/Kxz7s1S\nudNUp/dXJf0NWAKMBB4BupvZUDN7MOb5OxGKFqYsibalm0GUgKIniC6ECYJIaiWpDngHmGBmU2Ne\n17mScOCB0LEjPPVUoSPJr7/8Bb7/fdg8D2t3esLIndZNfPcz4J/AedE63kkZDVwjaTowkzCTfB2A\nma0H+kjaDnhAUi8ze7mhk9TU1Gx4X11dTXV1dYIhO5c7qYKERx1V6EjyY8UKGDMGZszIz/WqquDS\nS/NzrWJWW1tLbW1ti84ha6Q+QfTX/g5m9li97ccCS82s2ZwtqT9QY2aDo88XAtbUin2SXgf2N7OV\n9bZfDKwysz80cIw19nM4V+zeey+U4164ENq3L3Q0yfvLX+DJJ+Hee/NzvXXrwn1dvLgy7m9ckjCz\njArKNtWHMRpo6K/52cDvYp5/KtBDUldJbYGhwLj0HSS1k9Qmej8SmGRmKyV1lNQu2r4l8FV84SZX\nhjp2DE8Xd91V6EiSZxYWSUq6szvdZpuFpj/v+G65phLGtma2qP7GaFvHOCc3s3XAOcB4QqIZa2Zz\nJJ0l6cxot32AWZLmEEZTjYq2f4lQWv1F4HngCTN7NM51nSs1lVIqJNUiku8WY+/HyI2mmqReMbMe\nmX5XCN4k5Urd2rWw226hqWaffQodTXJOOQUGDgy1o/Lpjjvg0Udh7Nj8XreY5bpJaqKkX0sbV9dV\n8CuiYbDOudxo3RpOPbW852QsWRIS4qmn5v/aXuo8N5p6wtgauJkwl+LFaPOBwAvA9+t3SheSP2G4\ncjBnDgwaFFbja93U+MUSdfHF8OGH8Oc/5//aa9eGDu8334R27fJ//WKUzRNGo/8szWwVYWb2HoRZ\n2gCzzey1FsTonGvEPvtAly7wxBNw3HGFjia3Vq+Gm27a2IeRb61bwwEHQF1d/vtPykmzf8dECcKT\nhHN5kJqTkY+E8dFH8K9/5Wflv9mzYb/9YO/6hYHyKNXx7Qkje2X44Otc6Ro6FC64IKwSt8MOyV7r\nt7+FSZOgV69kr5Ny5ZX5uU5jqqpg/PjCxlDqGu3DKCXeh+HKyXe+A4ceCueem9w11q6Frl1h4sTy\nHpWVbubMMEpr3rxCR1IccjpKSlKHpl4tD9c515B8zMkYPz4M462UZAHhZ12yBD7+uNCRlK6mhtVO\nI4yImgYsIyyotCB67wPUnEvIwIGhXEiStZZuvTX0l1SS9I5vl52mFlDa3cz2ACYCXzezjma2A3A8\nYea2cy4Bm20Gp52W3JyM99+HCRNCf0ml8RnfLdPsehhA//SSHFExwsOSC8k5d8YZoaLr55/n/tx3\n3gnHHluZhfg8YbRMnITxlqRfSOoWvX4OvJV0YM5Vsh49whDURx7J/blvvTX0k1QiTxgtEydhDAN2\nBO6PXjtF25xzCUqi8/ull2DZstBPUol69QplzlesKHQkpcmH1TpXpFasCCOZ5s2DnXfOzTn/7/9g\n663h8stzc75S1L9/mBNy5JGFjqSwcloaRNJDNLGGtpmdkMmFnHOZ2XZbOOkk+Mc/4LzzWn6+zz8P\n/SL/+U/Lz1XKUs1SlZ4wstHUTO/f5y0K51yDhg8Piw395CegjP4W3NSjj4Z+kR5FszBBYVRVhaq5\nLnNNFR+clHofrZbXM/o4z8zWJB2Ycy78Ffzpp/DCC3DwwS07VyV3dqfr2xd+F3fNUPcFzfZhSKoG\nbgcWAgJ2A043s6eTDi4u78Nw5eyyy+Cdd8LSptlauhT22it0+G67be5iK0Vr1oQS5+++C9tsU+ho\nCifXCyilXAV8zcy+YmZHEpZRvTqbAJ1zmTvttLBS3GefZX+OMWNCf0ilJwuANm1C5dwXX2x+X/dF\ncRJGGzPbUK7LzOYDbZILyTmXrmtX6NMHHnwwu+PNvDmqPp+PkZ04CeMFSTdLqo5eNxFqTDnn8qQl\nczKmTYNPPoEjjshtTKXME0Z24iSMHwIvAz+OXi9H25xzeXLyyTBlSlhiNFO33gqnnw6t4vzfXiE8\nYWSnqTW9/21mgyRdYWYX5DmujHint6sEZ50F3brBz34W/5jPPoNOnWD69NC05YLPPw+1tJYtCxMZ\nK1GuO72/JOkw4ARJfST1TX+1LFTnXKZSy7dm8rfRuHGh/8OTxRe1bQv77usd35lqKmH8ErgY6Az8\ngTBaKvWKPalP0mBJcyXNl7TJk4qk9pLukzRD0mRJvaLtnSU9KWm2pJmSfpzJD+ZcuenfPzQrZTJT\n2zu7G+fNUpmLMw/jYjO7LKuTS60ICy8NIlS4nQoMNbO5aftcCawws8sk7QVcZ2ZHSdoF2MXMXpS0\nDWHRphPTj007hzdJuYpwxRXwyitw003N7/vmm7D//mGVua22Sj62UnPTTfDss3D77YWOpDASmYeR\nniwk1WQYUz9ggZktimaHjwVOrLdPL+DJ6FrzgG6SdjSzd8zsxWj7SmAO0CnD6ztXVk49Ff71L1i1\nqvl977gjrGHtyaJh/oSRuUzHTWRacLATsDjt8xI2/aU/AxgCIKkf0IXQDLaBpG5Ab+D5DK/vXFnZ\ndVc49FC4776m9zMLK/Z5c1Tj9tsPXnstXvJ1QVPFBxvSwvJnDRoNXCNpOjATqAPWbbhgaI66FxgV\nPWk0qKamZsP76upqqqurEwjVucI74wy4/vrwtNGY//43FCvs3z9vYZWctm3D+hgzZsBhFbCGaG1t\nLbW1tS06R0brYUhqZWbrM9i/P1BjZoOjzxcCZmZXNHHM68D+ZrZSUmvgYeAxM7umiWO8D8NVjNWr\nw1DZF14Iw2wbMnJkqEp7QVEPiC+8s84KTxrnnlvoSPIv530Yko6WdL2kcZLGAddJGpzB+acCPSR1\njSreDgXG1btGO0ltovcjgUlpTxJ/A15uKlk4V2k23xyGDWu8s3bVKrj33qafQFxQVRXmqLh4Gk0Y\nkv4IjAImAVdGr0nAjyXF+gVuZuuAc4DxwGxgrJnNkXSWpDOj3fYBZkmaQyhsOCq6/uHA/wADJdVJ\nmp5hsnKubA0fHvoo1jfwvH/ffaGfY9dd8x5Wyenb1zu+M9HUTO/5Ztazge0C5pvZnkkHF5c3SblK\nYwa9e8Mf/wgDBnzxu0GD4Ac/gG9+szCxlZLVq2H77eH992HLLQsdTX7luknqM0kNLdlyMNCCQsvO\nuZaSQuf3bbd9cfvChaET9wRfQDmWzTcPqxDOmFHoSEpDU6OkzgCul7QtYTgshMWTPoq+c84V0He/\nC7/6FaxYsXGdi9tvh6FDwy9CF09qPoaPKGteU0u0TgcOiWZcp+ZOvGlm7+QlMudck3bcEaqr4e67\nYcSI0J9x222hw9vFV1UVKgG75sWZ6f2OmU2LXu8ASNo7+dCcc81JXyfj6afDk0ZfLw2aEZ/xHV+2\nFfLH5zQK51xWjjkm1JZasGBjoUElMb22jO2/f7h/n35a6EiKX1OjpP7U2DHA6Wa2XWJRZchHSblK\ndt55sGZNqB01fz7stFOhIyo9ffrADTfAIYcUOpL8yWaUVFOd3sOB84DVDXw3LJOLOOeSM3x4+Cv5\nxBM9WWQr1SxVSQkjG00ljKnALDPbpPp+FlVrnXMJ2W8/GDgQfugLJ2etqiqUWnFNa6pJqgPwmZl9\nkt+QMudNUq7SmXnfRUtMmQJnnllZK/Bl0ySVUfHBYuUJwznXEp99Bh06wAcfwBZbFDqa/EhkASXn\nnCt3W2wBPXvCSy8VOpLi5gnDOefwyrVxxE4YknyhR+dc2fIJfM1rNmFIOkzSy8Dc6POBkv6SeGTO\nOZdHXuoF8mLuAAARZklEQVS8ec12ekt6HjgFGGdmfaJts8xsvzzEF4t3ejvnWurTT2GHHWD58soo\n3phYp7eZLa63aV2DOzrnXInacsuwrO3MmYWOpHjFSRiLJR0GmKQ2ks4H5iQcl3PO5Z33YzQtTsL4\nAXA2ocT5m0Dv6LNzzpUVTxhNa6o0CABm9h5hbW3nnCtrVVUby8W7TcXp9G6oau1HwAtm9mAiUWXI\nO72dc7nwySfQsWNldHwn1em9BaEZakH0OgDoDIyQ9MeMo3TOuSK11VbQvTvMmlXoSIpTs01ShARx\nuJmtA5B0PfAM8GXAxxM458pKqh+jqqrQkRSfOE8Y2wPbpH3eGugQJZCG1spwzrmS5R3fjYuTMK4E\nXpR0q6TbgDrgd5K2BiY2d7CkwZLmSpov6YIGvm8v6T5JMyRNltQr7btbJC2V5CXBnHN54QmjcbHK\nm0v6EtAv+jjVzN6KdXKpFTAfGAS8RViUaaiZzU3b50pghZldJmkv4DozOyr67svASuAOMzugiet4\np7dzLidSHd8ffght2xY6muQkWd78M+BtYDnQQ9KRMY/rBywws0VmtgYYC5xYb59ewJMAZjYP6CZp\nx+jzs9E1nXMuL7baCvbYA2bPLnQkxSdO8cHvA08DTwCXRv+tiXn+TkB6WZEl0bZ0M4Ah0bX6AV0I\no7Ccc64gvBBhw+KMkhoFHAxMNrMBkvYGfpPDGEYD10iaThh1VUcWtapqamo2vK+urqa6ujpH4Tnn\nKk2qH+P73y90JLlTW1tLbW1ti84RZ+LeVDM7WNKLwCFmtlrSbDPbt9mTS/2BGjMbHH2+EDAzu6KJ\nY14H9jezldHnrsBD3ofhnMuXZ5+Fn/wkrPVdrrLpw4jzhLFEUnvgAWCCpOXAopjnn0ro8+hK6AMZ\nCgxL30FSO+ATM1sjaSQwKZUsUrtEL+ecy4vevUMfxpo10KZNoaMpHnFqSZ0cva2R9BTQDng8zsnN\nbJ2kc4DxhP6SW8xsjqSzwtd2I7APcLuk9cBsYETqeEn/BKqBHSS9AVxiZl7pxTmXqG22ga5dQ9Lo\n3bvQ0RSPJpukJG0GzDazvfMXUua8Sco5l2unngrV1TBiRLO7lqScD6uNZnPPk9SlRZE551yJ8Ql8\nm4rTh7E9MFvSFGBVaqOZnZBYVM45V2BVVXDnnbk/74IF8PTTuT9vY04+GTp0yM254oyS+kpD281s\nUm5CaDlvknLO5drKlbDzzmHGdy47vo88ErbfPswmz4eaGthtt023JzJKyswmRaOc9jSziZK2AjbL\n5CLOOVdqttkGunSBl1+GAw/MzTnffjusGf7OO6W53kacmd4jgXuBv0abOhGG2DrnXFnLdT/GfffB\n8ceXZrKAeLWkzgYOBz4GMLMFwE5JBuWcc8WgqgqmT8/d+e69F045JXfny7c4CWO1mX2e+iCpNeAd\nBs65spfLJ4ylS6GuDo4+OjfnK4Q4CWOSpIuALSV9FbgHeCjZsJxzrvD69IGXXoK1a1t+rgcegGOP\nhS22aPm5CiVOwrgQWEYoDHgW8CjwiySDcs65YrDtttC5M8yZ0/JzlXpzFMSbh3ESYQGjm5IOxjnn\nik2qWWr//bM/x3vvhUKGDz6Yu7gKIc4TxteB+ZL+Lun4qA/DOecqQi76MR54IPRdbLVVbmIqlGYT\nhpkNB3oQ+i6GAa9KujnpwJxzrhjkImGUQ3MUxFzTG0BSG2AwMBw40szyNE+xeT7T2zmXlI8+gk6d\nwozv1lm0r3zwAey+O7z5ZpgMWCwSWdNb0jGSbgMWAN8AbgZ2ySpC55wrMe3awa67wty52R0/bhwc\ndVRxJYtsxenDOI0ws3svMzvDzB41sxwMMnPOudLQkmapcmmOgnh9GMPM7AEzWw0g6cuSrks+NOec\nKw7ZJoyPPoJnnoHjjst9TIUQ5wkDSX0k/U7SQuAyIMuHM+ecKz3ZJoxx48IiTNttl/OQCqLRLhxJ\nPQmjooYB7wF3ETrJB+QpNuecKwp9+8KMGbBuHWyWQa3ucmqOgiZGSUVrbD8DjDCzV6Jtr5nZHnmM\nLxYfJeWcS9qee4b5FPvuG2//jz8Os8TfeAPat082tmzkepTUEOBt4ClJN0kaBGR0cuecKxeZVq59\n5JGwWFIxJotsNZowoo7uocDewFPA/wI7Sbpe0tfyFaBzzhWDTPsxyq05CuKNklplZv80s68DnYE6\n4ILEI3POuSKSScJYuRImToQTTkg2pnyLPdO7mHkfhnMuacuXhyVbP/yw+Y7ve+6BW26Bxx/PT2zZ\nSGSmd0tJGixprqT5kjZ5MpHUXtJ9kmZImiypV9xjnXMuX7bfHnbaCebPb37fe+4pv+YoSDhhSGoF\nXAscDewLDJO0d73dLgLqzOxA4HTgTxkc65xzeROnWeqTT+CJJ+Ckk/ITUz4l/YTRD1hgZovMbA0w\nFjix3j69gCcBzGwe0E3SjjGPdc65vImTMB5/HPr1g45FU541d5JOGJ2AxWmfl0Tb0s0gDOFFUj+g\nC6FzPc6xzjmXN3ESRjmOjkophsWQRgPXSJpOWAa2DliX6Ulqamo2vK+urqa6ujpH4TnnXNC3L7z4\nIqxfD60a+HP7s8/g0Ufh6qvzH1tzamtrqa2tbdE5Eh0lJak/UGNmg6PPFwJmZlc0cczrwP7AfnGP\n9VFSzrl82WOPkBT2bqBHddy4kCyeeir/cWWqGEdJTQV6SOoqqS0wFBiXvoOkdtHiTEgaCUwys5Vx\njnXOuXxrqlmqXEdHpSSaMMxsHXAOMB6YDYw1szmSzpJ0ZrTbPsAsSXMII6JGNXVskvE651xzGksY\nq1fDww/DkCH5jylffOKec85lYMIEuPxymDTpi9sfeQSuuAKefrowcWWqGJuknHOurKR3fKcr59FR\nKZ4wnHMuAzvsAB06wCuvbNz2+eehw7ucm6PAE4ZzzmWsfj/GU0/BXnuF9S/KmScM55zLUN++X0wY\n5T46KsUThnPOZSj9CWPNmrAS3ze+UdiY8sEThnPOZSi1+t769WG0VPfu0LVroaNKnicM55zLUMeO\nYenVV1+tjNFRKcVQS8o550pOVRVMmQL33w///W+ho8kPf8JwzrksVFXBNdeEkVF77FHoaPLDE4Zz\nzmWhqgqmTq2c5ijw0iDOOZeVZcs2Ltm6556FjiZzXhrEOefyZMcdYfLk0kwW2fInDOecq0D+hOGc\ncy4xnjCcc87F4gnDOedcLJ4wnHPOxeIJwznnXCyeMJxzzsXiCcM551wsnjCcc87F4gnDOedcLIkn\nDEmDJc2VNF/SBQ18v52kcZJelDRT0hlp342Kts2U9OOkY3XOOde4RBOGpFbAtcDRwL7AMEl719vt\nbGC2mfUGBgBXSWotaV9gBHAQ0Bs4XlKFFBHOXm1tbaFDKAp+Hzbye7GR34uWSfoJox+wwMwWmdka\nYCxwYr19DNg2er8t8L6ZrQX2AZ43s9Vmtg54GhiScLwlz/+HCPw+bOT3YiO/Fy2TdMLoBCxO+7wk\n2pbuWqCXpLeAGcCoaPss4AhJ20vaCjgW2C3heJ1zzjWiGJZoPRqoM7OBkroDEyQdYGZzJV0BTABW\nAnXAukIG6pxzlSzR8uaS+gM1ZjY4+nwhYGZ2Rdo+DwO/NbPnos//Bi4wsxfqnevXwGIzu6GB63ht\nc+ecy1Cm5c2TfsKYCvSQ1BV4GxgKDKu3zyLgKOA5STsDPYHXACTtaGbLJHUBTgb6N3SRTH9o55xz\nmUs0YZjZOknnAOMJ/SW3mNkcSWeFr+1G4HLgNkkvRYf9PzP7IHr/L0kdgDXAj8zs4yTjdc4517iy\nWHHPOedc8kp6pndzkwLLmaRbJC1NezIjGlE2XtI8SU9IalfIGPNFUmdJT0qanT7JsxLvh6TNJT0v\nqS66F5dE2yvuXkCYCyZpuqRx0eeKvA8AkhZKmhH925gSbcvofpRswog5KbCc3Ur42dNdCEw0s72A\nJ4Gf5T2qwlgL/MTM9gUOBc6O/i1U3P0ws9XAADPrQ5jweoykflTgvYiMAl5O+1yp9wFgPVBtZn3M\nrF+0LaP7UbIJg3iTAsuWmT0LLK+3+UTg9uj97cBJeQ2qQMzsHTN7MXq/EpgDdKZy78cn0dvNCf2U\nRgXeC0mdCfO3bk7bXHH3IY3Y9Hd+RvejlBNGnEmBlWYnM1sK4ZcosFOB48k7Sd0If1lPBnauxPsR\nNcPUAe8AE8xsKpV5L64GfkpImCmVeB9SjDDPbaqk70fbMrofxTBxzyWnokY0SNoGuBcYZWYrG5if\nUxH3w8zWA30kbQfcH9Vlq6h7Iek4YKmZvSipuoldy/o+1HO4mb0taUdgvKR5ZPjvopSfMN4EuqR9\n7hxtq2RLo7ksSNoFeLfA8eSNpNaEZPF3M3sw2lyx9wMgGoZeCwym8u7F4cAJkl4D7gQGSvo78E6F\n3YcNzOzt6L/LgAcIzfoZ/bso5YSxYVKgpLaESYHjChxTvil6pYwDzojenw48WP+AMvY34GUzuyZt\nW8XdD0kdUyNdJG0JfJXQp1NR98LMLjKzLma2B+F3w5NmdirwEBV0H1IkbRU9gSNpa+BrwEwy/HdR\n0vMwJA0GrmHjpMDRBQ4pbyT9E6gGdgCWApcQ/mq4h1CkcRHwLTP7sFAx5oukwwnVjGcSHqkNuAiY\nAtxNBd0PSfsTOi9bRa+7zOzX0QTYiroXKZK+ApxnZidU6n2QtDtwP+H/jdbAGDMbnen9KOmE4Zxz\nLn9KuUnKOedcHnnCcM45F4snDOecc7F4wnDOOReLJwznnHOxeMJwzjkXiycMVzKiEuZfrbdtlKTr\nmjluRY6u31HSZEnTorkf2ZzjKUl9m9nnxkwqL0u6RNJPGth+m6QlktpEn3eQ9Hr0vquk9ZLOTtv/\nz5JOi//TuErjCcOVkn+y6RK/Q6PtTcnVZKOjgJfMrCq1Bn1zojL8GTGzM81sbsbRNXAqQun379Xb\nlvIuMCoqq+JcszxhuFLyL+DY1C84hbXiv2Rmz0naWtJESS9Ei8ScUP9gSV+R9FDa5w1/UUvqK6k2\nquT5WKq+Ttq+BwJXACdFC/JsLmmYpJei1+i0fVdI+n1UMbbBdegV3CrpVw18t+EpJDrX5ZJelPSf\nqHBcoySNlPSIpM2jTX8E/q+RxLUM+DcbS0M41yRPGK5kmNlyQrmPY6JNQwllDQA+A04ys4OAgcBV\njZ2m/oYoAf0Z+IaZHUxYnOo39a49A/glMNbM+gIdgNGE8iy9gYPTktTWwH+jhWr+00AMbYAxwHwz\n+2UzP/bWwH/MrDfwDDCykf0UNS8dC5wYLaQE8AbwLHBqA8cYIQmeL0kNfO/cF3jCcKVmLCFREP33\nzui9gN9KmgFMBHaVFHetg72A/QhrBdQBPwd2beaYg4GnzOyDqJz4GODI6Lt1wH1NHPtXYKaZ/TZG\nbKvN7NHo/TSgWyP7nUaoSnuKma2t991owroQrfhisUrMbCFh7ZD/iRGLq3CeMFypeRAYJKkPsKWZ\n1UXb/wfoCPSJlid9F9ii3rFr+eK/+dT3AmaZWd/oqeBAMzuG5jX2V/mn1nSRtueAAWnNRk1Zk/Z+\nHY2vYfMSIZnsVv8LM3sFeBH4Fg335/wWuCBGLK7CecJwJcXMVhHWePgbG58uANoB75rZekkDgK5p\n36V+sS8CeklqI6k9MCjaPg/YUVJ/CE1Ukno1E8oU4EhJHSRtRuiMr613vcbcAjwK3B0d25S4TUV1\nwFnAuGhdg/p+A5zf0LnNbB5h3etN+n2cS+cJw5WiO4ED+GLCGEPoR5gBfJewBkSKAZjZEkKfxyxC\n09b0aPsa4BTgCkkvEn75HtpUANFylhcSkkQd8IKZPZx+vcYOjY7/Y3TcHY3tE+Nc9WP6DyEpPBKV\nrba0714m/LyNnfvX+BLHrhle3tw551ws/oThnHMuFk8YzjnnYvGE4ZxzLhZPGM4552LxhOGccy4W\nTxjOOedi8YThnHMuFk8YzjnnYvn/MJ8d3dqs3pwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25e2d4284a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "\n",
    "plt.plot(k_candidates, average_accuracies)\n",
    "plt.xlabel(\"Value for k in kNN\")\n",
    "plt.ylabel(\"Average 10-fold CV Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the optimal performance is between 5 and 20, but the best value we achieved was at ``k=15``, with an accuracy of 96%.\n",
    "\n",
    "Now, let's train our final classifier on all the training data with ``k=15`` and see how well it performs on the held-out test data, for the first - and only- time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Set Accuracy:\n",
      "0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=15)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Final Test Set Accuracy:\")\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "98% accuracy! Our 15-NN model generalized quite well, and since we used cross-validation to find the best value of k, we can be absolutely certain that the test data is truly unseen data. Here's the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17,  0,  0],\n",
       "       [ 0, 16,  1],\n",
       "       [ 0,  0, 16]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 50 training examples, every one was classified correctly, except for one instance of *I. versicolor* that was erroneously classified as *I. virginica*. Not bad, for data the model has never seen. \n",
    "\n",
    "# Not just KNN\n",
    "\n",
    "Here, I used k-NN as an example, with the hyperparameter ``k`` that needs to be optimized. But you can use this cross validation to optimize *any* model that uses hyperparameters. For example, there's the extremely important hyperparameter in SVM called ``C``. Generate some candidates for ``C`` and find k-fold CV accuracies for each value. Choose a value for ``C`` that gets the highest accuracy.\n",
    "\n",
    "Recall that SVM also had a non-numeric hyperparameter: the type of kernel you can use, linear, rbf, or poly. The same principle applies here. Do k-fold CV for each of the kernels, and find the one that has the best average accuracy."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
