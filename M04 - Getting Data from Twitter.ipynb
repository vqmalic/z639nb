{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'm going to save my API Key and API secret in some variables here. Keep these secret! If you're working with a partner and are using Twitter data, I highly recommending creating a new Twitter account *for the project* and making an application using that account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "API_KEY = \"yEuseuXxD2jAGA7GS2jU4UAB7\"\n",
    "API_SECRET = \"cnV5DQ6zOIf8x9LHT9xo8qmOVZDeDcI1T1k49XQHcAIoGvrizo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and import tweepy and get our API object set up. First, we need to make an auth object using tweepy's method ``AppAuthHandler``. It takes the ``API_KEY`` and the ``API_SECRET`` as its arguments. \n",
    "\n",
    "Then, we call the tweepy ``API`` method. Like with scikit-learn, consider this method to be a **factory** method that makes the api object we'll actually use. It takes the ``auth`` object we created as its argument; it uses these credentials to access Twitter.\n",
    "\n",
    "I'm passing two additional arguments to the ``API`` method. I'm setting ``wait_on_rate_limit=True``, which means that if during the process of getting your data, you hit your rate limit, Tweepy will handle everything necessary to wait the correct amount of time *and* resume whatever it was doing right where it left off. I'm also setting ``wait_on_rate_limit_notify=True`` so you'll be notified when Tweepy is waiting, which is better than staring at at the \"still processing\" asterisk without knowing what's going on.\n",
    "\n",
    "I'm going to save the API object in the variable ``api`` for easy reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "auth = tweepy.AppAuthHandler(API_KEY, API_SECRET)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Tweepy User Object\n",
    "\n",
    "Let's load an arbitrary user into our environment and see what information we can obtain about him. Our lucky experimental subject will be IU Bloomington, whose handle is IUBloomginton.. Feel free to replace me with your own account or that of your favorite Tweeter if you'd like.\n",
    "\n",
    "Getting a user involves calling the method of our ``api``called ``get_user``. It takes one argument: the user id of the user you want to grab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user = api.get_user(\"IUBloomington\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we have a user saved in the variable ``user``. What information do we have access to? A lot, actually. As always, we can access them through dot notation. \n",
    "\n",
    "As you know, with dot notation, we can access the *attributes* (e.g. variables) and *methods* (e.g functions) that belong to an object. We've been using this notation a lot with scikit-learn. Sometimes, you don't have any idea what attributes or methods an object has. There are two ways to resolve this problem. One is to look up the API documentation for, in this scenario, Tweepy. The other is to use the handy ``dir`` function that is built into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_api',\n",
       " '_json',\n",
       " 'contributors_enabled',\n",
       " 'created_at',\n",
       " 'default_profile',\n",
       " 'default_profile_image',\n",
       " 'description',\n",
       " 'entities',\n",
       " 'favourites_count',\n",
       " 'follow',\n",
       " 'follow_request_sent',\n",
       " 'followers',\n",
       " 'followers_count',\n",
       " 'followers_ids',\n",
       " 'following',\n",
       " 'friends',\n",
       " 'friends_count',\n",
       " 'geo_enabled',\n",
       " 'has_extended_profile',\n",
       " 'id',\n",
       " 'id_str',\n",
       " 'is_translation_enabled',\n",
       " 'is_translator',\n",
       " 'lang',\n",
       " 'listed_count',\n",
       " 'lists',\n",
       " 'lists_memberships',\n",
       " 'lists_subscriptions',\n",
       " 'location',\n",
       " 'name',\n",
       " 'notifications',\n",
       " 'parse',\n",
       " 'parse_list',\n",
       " 'profile_background_color',\n",
       " 'profile_background_image_url',\n",
       " 'profile_background_image_url_https',\n",
       " 'profile_background_tile',\n",
       " 'profile_banner_url',\n",
       " 'profile_image_url',\n",
       " 'profile_image_url_https',\n",
       " 'profile_link_color',\n",
       " 'profile_location',\n",
       " 'profile_sidebar_border_color',\n",
       " 'profile_sidebar_fill_color',\n",
       " 'profile_text_color',\n",
       " 'profile_use_background_image',\n",
       " 'protected',\n",
       " 'screen_name',\n",
       " 'status',\n",
       " 'statuses_count',\n",
       " 'time_zone',\n",
       " 'timeline',\n",
       " 'translator_type',\n",
       " 'unfollow',\n",
       " 'url',\n",
       " 'utc_offset',\n",
       " 'verified']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there's a huge amount of information you can pull once you've created a user object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the user:\n",
      "Indiana University\n",
      "The user's location:\n",
      "Bloomington, IN\n",
      "The user's time zone:\n",
      "Eastern Time (US & Canada)\n",
      "The amount of accounts this user follows:\n",
      "629\n",
      "The amount of accounts who follow this user:\n",
      "210552\n"
     ]
    }
   ],
   "source": [
    "print(\"The name of the user:\")\n",
    "print(user.name)\n",
    "print(\"The user's location:\")\n",
    "print(user.location)\n",
    "print(\"The user's time zone:\")\n",
    "print(user.time_zone)\n",
    "print(\"The amount of accounts this user follows:\")\n",
    "print(user.friends_count)\n",
    "print(\"The amount of accounts who follow this user:\")\n",
    "print(user.followers_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so on. Note the methods ``followers`` and ``friends``. These are **methods** so require parenthesis after them.  They return iterables, that themselves contain the ``user`` objects of the people who follow and are followed by the chosen user, respectively. Here, I'm iterating through the first 10 followers of IUBloomginton and printing their name and account description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arman Reihani\n",
      "//üá∫üá∏//üáÆüá∑//\n",
      "Brother, son, student of life. Berlin born, Bloomington bred.\n",
      "Let us wander where the WiFi is weakest.\n",
      "**************************************************\n",
      "Chicke Duncan\n",
      "\n",
      "**************************************************\n",
      "Jennifer McCormick\n",
      "\n",
      "**************************************************\n",
      "Cameron Miller\n",
      "\n",
      "**************************************************\n",
      "Kelly Murawski\n",
      "an indie girl in an indy world *\n",
      "being the light * \n",
      "happily married to my bff * \n",
      "makeup junkie * \n",
      "once a writer always a writer *\n",
      "music fanatic *\n",
      "sassy pants\n",
      "**************************************************\n",
      "x3x29\n",
      "\n",
      "**************************************************\n",
      "Kay Robertson\n",
      "üñ§made a Twitter cause IU forgot R and Q on their alphabetized Dean's Listüñ§\n",
      "‚òÄÔ∏è90k in debt‚òÄÔ∏è\n",
      "**************************************************\n",
      "Sydney flatt\n",
      "\n",
      "**************************************************\n",
      "Jennifer Miller\n",
      "ÈõªÂΩ± ÁßëÊäÄ ÊµÅË°åÊñáÂåñ ÁßëÂ≠∏ ÊîøÂ∫úËàáÊîøÊ≤ª\n",
      "**************************************************\n",
      "Brian Cygnar\n",
      "Autumn moons and winter skies Snap: bcygnar\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "followers = user.followers()\n",
    "for f in followers[:10]:\n",
    "    print(f.name)\n",
    "    print(f.description)\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't started our study of networks yet, but you could easily make a follower-following network using only the commands you know so far. Start, for example, with your own user account. Then, get all your followers. Then get all *their* followers. And so on. \n",
    "\n",
    "# The Tweepy Status Object\n",
    "\n",
    "First, note that in Twitter's API and in Tweepy the tweets are actually referred to as **statuses**. Don't get thrown off by this terminology!\n",
    "\n",
    "We can obtain a specific tweet using the ``api`` object's ``get_status`` object. It takes one argument: a statuses' numeric ID.\n",
    "\n",
    "Note that I'm doing this for instructional purposes, so we can first explore what a Status object is like. More often than not, you won't be getting tweets by directly referencing their numeric ID. You'll probably going through a list of users or hashtags and *then* getting the tweets associated with them. We'll cover that shortly.\n",
    "\n",
    "I'm grabbing a tweet from IUBloomington, which is actually a retweet from IU Basketball's account. I got this ID manually by looking at the tweet's URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "status = api.get_status(773643148878807040)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the handy ``dir`` method to see what information we have available to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_api',\n",
       " '_json',\n",
       " 'author',\n",
       " 'contributors',\n",
       " 'coordinates',\n",
       " 'created_at',\n",
       " 'destroy',\n",
       " 'entities',\n",
       " 'extended_entities',\n",
       " 'favorite',\n",
       " 'favorite_count',\n",
       " 'favorited',\n",
       " 'geo',\n",
       " 'id',\n",
       " 'id_str',\n",
       " 'in_reply_to_screen_name',\n",
       " 'in_reply_to_status_id',\n",
       " 'in_reply_to_status_id_str',\n",
       " 'in_reply_to_user_id',\n",
       " 'in_reply_to_user_id_str',\n",
       " 'is_quote_status',\n",
       " 'lang',\n",
       " 'parse',\n",
       " 'parse_list',\n",
       " 'place',\n",
       " 'possibly_sensitive',\n",
       " 'possibly_sensitive_appealable',\n",
       " 'retweet',\n",
       " 'retweet_count',\n",
       " 'retweeted',\n",
       " 'retweets',\n",
       " 'source',\n",
       " 'source_url',\n",
       " 'text',\n",
       " 'truncated',\n",
       " 'user']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we have a huge amount of information available to us. Most basic, of course, is the status text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squad üíØ #iubb https://t.co/D5DypL976H\n"
     ]
    }
   ],
   "source": [
    "print(status.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the Trump Twitter analysis from our first lecture. Statuses have a ``source`` attribute that indicates the device that made the tweet. It looks like IU Basketball simply used the online Twitter web client, but this would also indicate, for example, if the tweet came from an IPhone or Android."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter Web Client\n"
     ]
    }
   ],
   "source": [
    "print(status.source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other attributes you can see here that may serve as useful features for a machine learning algorithm. ``retweet_count`` and ``favorite_count``, for example, can be considered proxies for how popular a status is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retweets:\n",
      "627\n",
      "Number of times favorited:\n",
      "1260\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of retweets:\")\n",
    "print(status.retweet_count)\n",
    "print(\"Number of times favorited:\")\n",
    "print(status.favorite_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, this status is generating a lot of excitement. \n",
    "\n",
    "As with the Tweepy ``user`` object, there are methods that allow us to jump to *other* ``user`` and ``status`` objects. This is where the API really gets powerful, and where you can be computationally mobile by jumping from place to place. \n",
    "\n",
    "The simplest attribute is the ``author`` attribute. Note carefully that this is the ``user`` object of the account that created the Tweet. So, printing just ``status.author`` would give us a bunch of hard-to-read mumbo-jumbo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User(profile_text_color='333333', time_zone='Eastern Time (US & Canada)', translator_type='none', profile_background_tile=False, id_str='791598918', profile_use_background_image=True, has_extended_profile=False, protected=False, default_profile=False, profile_image_url='http://pbs.twimg.com/profile_images/907637855278727169/0BYciSu6_normal.jpg', lang='en', profile_sidebar_border_color='C0DEED', is_translator=False, utc_offset=-18000, listed_count=839, _api=<tweepy.api.API object at 0x00000269504C6AC8>, profile_link_color='990000', created_at=datetime.datetime(2012, 8, 30, 13, 29, 31), follow_request_sent=None, screen_name='IndianaMBB', notifications=None, description=\"The official account of Indiana Men's Basketball. #IUBB\", friends_count=221, name='Indiana Basketball', profile_background_image_url='http://pbs.twimg.com/profile_background_images/646712891/m9bpi9c2fpy3a9wn59d5.jpeg', id=791598918, profile_image_url_https='https://pbs.twimg.com/profile_images/907637855278727169/0BYciSu6_normal.jpg', profile_background_image_url_https='https://pbs.twimg.com/profile_background_images/646712891/m9bpi9c2fpy3a9wn59d5.jpeg', followers_count=1090090, profile_sidebar_fill_color='DDEEF6', profile_background_color='C0DEED', favourites_count=6558, location='Bloomington, IN', verified=True, profile_banner_url='https://pbs.twimg.com/profile_banners/791598918/1517703129', entities={'url': {'urls': [{'indices': [0, 23], 'expanded_url': 'http://IUHoosiers.com/MBasketball', 'url': 'https://t.co/DEKIvNQfvw', 'display_url': 'IUHoosiers.com/MBasketball'}]}, 'description': {'urls': []}}, statuses_count=14433, is_translation_enabled=False, url='https://t.co/DEKIvNQfvw', contributors_enabled=False, _json={'profile_background_color': 'C0DEED', 'profile_link_color': '990000', 'profile_background_image_url_https': 'https://pbs.twimg.com/profile_background_images/646712891/m9bpi9c2fpy3a9wn59d5.jpeg', 'translator_type': 'none', 'utc_offset': -18000, 'profile_background_image_url': 'http://pbs.twimg.com/profile_background_images/646712891/m9bpi9c2fpy3a9wn59d5.jpeg', 'profile_background_tile': False, 'id_str': '791598918', 'profile_use_background_image': True, 'favourites_count': 6558, 'has_extended_profile': False, 'default_profile': False, 'contributors_enabled': False, 'time_zone': 'Eastern Time (US & Canada)', 'profile_text_color': '333333', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/791598918/1517703129', 'profile_image_url': 'http://pbs.twimg.com/profile_images/907637855278727169/0BYciSu6_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/907637855278727169/0BYciSu6_normal.jpg', 'lang': 'en', 'profile_sidebar_border_color': 'C0DEED', 'description': \"The official account of Indiana Men's Basketball. #IUBB\", 'is_translator': False, 'id': 791598918, 'location': 'Bloomington, IN', 'verified': True, 'entities': {'url': {'urls': [{'indices': [0, 23], 'expanded_url': 'http://IUHoosiers.com/MBasketball', 'url': 'https://t.co/DEKIvNQfvw', 'display_url': 'IUHoosiers.com/MBasketball'}]}, 'description': {'urls': []}}, 'listed_count': 839, 'followers_count': 1090090, 'is_translation_enabled': False, 'url': 'https://t.co/DEKIvNQfvw', 'protected': False, 'created_at': 'Thu Aug 30 13:29:31 +0000 2012', 'geo_enabled': True, 'follow_request_sent': None, 'default_profile_image': False, 'screen_name': 'IndianaMBB', 'notifications': None, 'following': None, 'profile_sidebar_fill_color': 'DDEEF6', 'statuses_count': 14433, 'friends_count': 221, 'name': 'Indiana Basketball'}, geo_enabled=True, following=False, default_profile_image=False)\n"
     ]
    }
   ],
   "source": [
    "print(status.author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, you have to treat ``status.author`` as a ``user`` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The official account of Indiana Men's Basketball. #IUBB\n"
     ]
    }
   ],
   "source": [
    "author_object = status.author\n",
    "print(author_object.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can skip the assignment step, and just use dot notation twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The official account of Indiana Men's Basketball. #IUBB\n"
     ]
    }
   ],
   "source": [
    "print(status.author.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, reading this as normal English. Starting with the **status**, obtain (with dot notation) its **author**, then, from **that user object**, obtain (with dot notation) its description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then there is the ``retweets`` method, which returns an iterable of all the **retweets of this status**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @IndianaMBB: Squad üíØ #iubb https://t.co/D5DypL976H\n",
      "RT @IndianaMBB: Squad üíØ #iubb https://t.co/D5DypL976H\n",
      "RT @IndianaMBB: Squad üíØ #iubb https://t.co/D5DypL976H\n",
      "RT @IndianaMBB: Squad üíØ #iubb https://t.co/D5DypL976H\n",
      "RT @IndianaMBB: Squad üíØ #iubb https://t.co/D5DypL976H\n"
     ]
    }
   ],
   "source": [
    "retweets = status.retweets()\n",
    "for retweet in retweets[:5]:\n",
    "    print(retweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's neither helpful nor surprising. Retweets are often just verbatim the text of the original tweet (plus an RT at the start). What *could* be helpful, depending on your analysis, is *who* is retweeting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who:\n",
      "RonShabazz_02\n",
      "When:\n",
      "2016-10-27 15:08:08\n",
      "**************************************************\n",
      "Who:\n",
      "_rocketsss\n",
      "When:\n",
      "2016-10-26 21:31:26\n",
      "**************************************************\n",
      "Who:\n",
      "Casey_Smith9\n",
      "When:\n",
      "2016-10-07 20:26:30\n",
      "**************************************************\n",
      "Who:\n",
      "CarlousThrell\n",
      "When:\n",
      "2016-10-03 05:14:37\n",
      "**************************************************\n",
      "Who:\n",
      "03Brayanm\n",
      "When:\n",
      "2016-10-03 05:13:33\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for retweet in retweets[:5]:\n",
    "    print(\"Who:\")\n",
    "    print(retweet.author.screen_name)\n",
    "    print(\"When:\")\n",
    "    print(retweet.created_at)\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we wanted to, we could keep this going. We could get the tweets of one of these users, and see who retweeted those tweets, and so and and so forth. \n",
    "\n",
    "Hopefully at this point you can see that you can easily jump from a ``user`` object to a ``status`` object, and visa-versa. This is one very versatile way to *collect data* for your algorithms. How you do so, of course, will depend heavily on what your goal is, but the tools are available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching\n",
    "\n",
    "Many of you will want to seed your data collection with a search term rather than a particular user or Tweet. We can easily do this using a method belonging to a Tweepy ``api`` called ``search``. \n",
    "\n",
    "The ``search`` method takes an argument called ``q`` (standing for \"query\") that simply contains the string you want to search for. Hashtags work here as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#iufb has had 3 former players play in the Super Bowl the last 3 years. 2 of the 3 are Super Bowl Champs. A former‚Ä¶ https://t.co/29YL5cqXkQ\n",
      "2018-02-05 04:12:12\n",
      "**************************************************\n",
      "RT @IUeyeDOC: It‚Äôs amazing Archie has this roster, the worst shooting squad in IU history, competitive in any game. #SendShootersFast #IUBB\n",
      "2018-02-05 04:06:07\n",
      "**************************************************\n",
      "#CampusOfChampions #iubb #iufb #iubase https://t.co/ExQMnCPvWg\n",
      "2018-02-05 03:57:58\n",
      "**************************************************\n",
      "RT @JeffRabjohns: Zach McRoberts, best walk-on in major-college basketball???  #iubb https://t.co/ccmJtwynAo\n",
      "2018-02-05 03:33:19\n",
      "**************************************************\n",
      "RT @BtownBanners: LOL at Archie #iubb https://t.co/V5vfkRvXKG\n",
      "2018-02-05 03:24:44\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "search_results = api.search(q=\"#IUBB\")\n",
    "\n",
    "for status in search_results[:5]:\n",
    "    print(status.text)\n",
    "    print(status.created_at)\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other useful parameters to the search method. The ``locale`` method takes a two-letter code representing the language you want returned. Let's say I'm interested in the hashtag #rio, but I want English results, not Portuguese or other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#RockInRio #Rock #Rio #Skank #Rock SHOW https://t.co/vSC1fywmN6 via @YouTube\n",
      "**************************************************\n",
      "#Beautiful #Rio Cum Girls #Hereford https://t.co/pfYTFGX0ZY\n",
      "**************************************************\n",
      "#AcompanhanteRJ #EscortGirlRJ #MulherRJ #GarotaDeProgramaRJ #EscortGirlRioDeJaneiro #BrazilianWoman #FemaleEscort‚Ä¶ https://t.co/wuIZSJPsW2\n",
      "**************************************************\n",
      "Well at least Gisele got the moment of her life #rio #bradyfamily\n",
      "**************************************************\n",
      "RT @DLBiller: Crypto makes its #Rio Carnaval debut. #bitcoin https://t.co/x9M1RLQITz\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "search_results = api.search(q=\"#rio\", lang=\"en\")\n",
    "\n",
    "for status in search_results[:5]:\n",
    "    print(status.text)\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(when I ran the code for this notebook, a Turkish-language status slipped in - the locale parameter isn't always perfect, so there's always preprocessing before analysis!)\n",
    "\n",
    "There's also a ``geocode`` parameter, a string that takes the following format``\"latitude,longitude,radius[km|mi]\"``. Let's see what people within 20km of Paris are saying about #rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @cwillem: #newsingle #restart #rio #NouvellePhotoDeProfil https://t.co/fDD5ctRbad\n",
      "**************************************************\n",
      "RT @cwillem: #newsingle #restart #rio #NouvellePhotoDeProfil https://t.co/fDD5ctRbad\n",
      "**************************************************\n",
      "RT @cwillem: #newsingle #restart #rio #NouvellePhotoDeProfil https://t.co/fDD5ctRbad\n",
      "**************************************************\n",
      "RT @cwillem: Hey un clin d‚Äô≈ìil de #Rio d√©j√† en mode Carnaval :-) j‚Äôesp√®re que vous allez tous bien ü§ì https://t.co/XuPLNG4Ii3\n",
      "**************************************************\n",
      "RT @cwillem: Lover after all #Rio vous et moi un jour j‚Äôy crois. Belle nuit üòò https://t.co/e8CSPh6i9u\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "search_results = api.search(q=\"#rio\", geocode=\"48.86,2.35,20km\")\n",
    "\n",
    "for status in search_results[:5]:\n",
    "    print(status.text)\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trends\n",
    "\n",
    "Let's say you don't have a starting search term, user, or Tweet. What if you're just interested in what's popular among users *right now?* Perhaps these will inspire you to find themes to narrow down your search. A Tweepy ``api`` object has a method called ``trends_place`` that can tell you what terms are currently trending. The method takes one argument, an *id representing the place that you want to see trends from*. These IDs are from Yahoo's Where On Earth ID (WOEID) system. [Here is a website where you can look up the corresponding WOEID for a given place](http://woeid.rosselliot.co.nz/). For starters, the United States is 23424977 and the World is 1.\n",
    "\n",
    "The object returned by ``trends_place`` is a tricky JSON object. It's a little harder to parse than the ``user`` and ``status`` objects. I'll guide you through this step by step. First, let's see what's trending in the US. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trending_us = api.trends_place(23424977)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you print out the trending_us variable, you will see a *lot* of stuff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'trends': [{'url': 'http://twitter.com/search?q=%23SuperBowl', 'tweet_volume': 2159116, 'promoted_content': None, 'query': '%23SuperBowl', 'name': '#SuperBowl'}, {'url': 'http://twitter.com/search?q=%23ThisIsUs', 'tweet_volume': 89763, 'promoted_content': None, 'query': '%23ThisIsUs', 'name': '#ThisIsUs'}, {'url': 'http://twitter.com/search?q=%22Kevin+Hart%22', 'tweet_volume': 57767, 'promoted_content': None, 'query': '%22Kevin+Hart%22', 'name': 'Kevin Hart'}, {'url': 'http://twitter.com/search?q=Prince', 'tweet_volume': 475312, 'promoted_content': None, 'query': 'Prince', 'name': 'Prince'}, {'url': 'http://twitter.com/search?q=%23CloverfieldParadox', 'tweet_volume': 23992, 'promoted_content': None, 'query': '%23CloverfieldParadox', 'name': '#CloverfieldParadox'}, {'url': 'http://twitter.com/search?q=%22Chris+Collinsworth%22', 'tweet_volume': 24102, 'promoted_content': None, 'query': '%22Chris+Collinsworth%22', 'name': 'Chris Collinsworth'}, {'url': 'http://twitter.com/search?q=%22Eli+Manning%22', 'tweet_volume': 47048, 'promoted_content': None, 'query': '%22Eli+Manning%22', 'name': 'Eli Manning'}, {'url': 'http://twitter.com/search?q=%22Jesse+James%22', 'tweet_volume': 13728, 'promoted_content': None, 'query': '%22Jesse+James%22', 'name': 'Jesse James'}, {'url': 'http://twitter.com/search?q=%22Malcolm+Butler%22', 'tweet_volume': 19915, 'promoted_content': None, 'query': '%22Malcolm+Butler%22', 'name': 'Malcolm Butler'}, {'url': 'http://twitter.com/search?q=%22Darrell+Green%22', 'tweet_volume': None, 'promoted_content': None, 'query': '%22Darrell+Green%22', 'name': 'Darrell Green'}, {'url': 'http://twitter.com/search?q=%22Steven+Tyler%22', 'tweet_volume': None, 'promoted_content': None, 'query': '%22Steven+Tyler%22', 'name': 'Steven Tyler'}, {'url': 'http://twitter.com/search?q=%22All+Apologies%22', 'tweet_volume': None, 'promoted_content': None, 'query': '%22All+Apologies%22', 'name': 'All Apologies'}, {'url': 'http://twitter.com/search?q=%23InfinityWar', 'tweet_volume': 211122, 'promoted_content': None, 'query': '%23InfinityWar', 'name': '#InfinityWar'}], 'locations': [{'woeid': 23424977, 'name': 'United States'}], 'as_of': '2018-02-05T04:14:03Z', 'created_at': '2018-02-05T04:12:01Z'}]\n"
     ]
    }
   ],
   "source": [
    "print(trending_us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not super helpful. If you stared at this for a while, you could gradually parse what's going on. To make things easy, I'll just skip the messy stuff and inform you that:\n",
    "* ``trending_us`` is a list with one object\n",
    "* the one object in ``trending_us`` is a dictionary\n",
    "* most of the stuff we're interested in is in the dictionary at the key ``trends``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trends_usa_useful = trending_us[0]['trends']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're getting closer to useful data, but we're not 100% there yet. Here's what you need to know about the new ``trends_usa_useful`` variable.\n",
    "* ``trends_usa_useful`` is a *list of dictionaries*\n",
    "* Each dictionary in the list represents a trend\n",
    "* Each dictionary has teh following keys\n",
    " * ``tweet_volume``: how many tweets have this trend\n",
    " * ``query``: the query string for the trend\n",
    " * ``name``: the name of the trend\n",
    " * ``promoted_content``: whether or not this trend was promoted by a business\n",
    " * ``url``: the full URL for searching this trend\n",
    "Let's isolate the very first trend and see what it's about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:\n",
      "#SuperBowl\n",
      "Tweet volume:\n",
      "2159116\n",
      "Query:\n",
      "%23SuperBowl\n",
      "URL:\n",
      "http://twitter.com/search?q=%23SuperBowl\n",
      "Promoted?:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "first_trend = trends_usa_useful[0]\n",
    "print(\"Name:\")\n",
    "print(first_trend['name'])\n",
    "print(\"Tweet volume:\")\n",
    "print(first_trend['tweet_volume'])\n",
    "print(\"Query:\")\n",
    "print(first_trend['query'])\n",
    "print(\"URL:\")\n",
    "print(first_trend['url'])\n",
    "print(\"Promoted?:\")\n",
    "print(first_trend['promoted_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, using Tweepy the only things you really need are tweet volume and name. After all, you can just use the name as the value for the `q` parameter in `api.search` if you want to generate a list of Tweets. Here are the 5 trends in the US at the moment, and how big they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#SuperBowl 2159116\n",
      "#ThisIsUs 89763\n",
      "Kevin Hart 57767\n",
      "Prince 475312\n",
      "#CloverfieldParadox 23992\n"
     ]
    }
   ],
   "source": [
    "for trend in trends_usa_useful[:5]:\n",
    "    print(trend['name'], trend['tweet_volume'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** the tweet volume is for the last 24 hours, you're likely to see trends with a tweet volume of ``None`` if the data for them hasn't been collected yet. This means the trend is relatively recent.\n",
    "\n",
    "Since the trends objects are tricky, I broke that down into steps so you could follow along. Here's me finding out what's trending in Russia, but in more compact code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#SuperBowl\n",
      "–ö–∞–π–ª–∏\n",
      "#–ì—ç—Ä—Ä–∏–§–æ–ª–ª–æ–≤–∏—Ç–í—Å–µ—Ö\n",
      "–°—É-25\n",
      "#TwitterBestFandom\n"
     ]
    }
   ],
   "source": [
    "trends_russia = api.trends_place(23424936)\n",
    "\n",
    "for trend in trends_russia[0]['trends'][:5]:\n",
    "    print(trend['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating through Large Lists in Tweepy\n",
    "\n",
    "Notice that whenever we call for a *list of something* in Tweety, there's a limit to what you can get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "search_results = api.search(q=\"#StarTrek50\")\n",
    "print(len(search_results))\n",
    "user = api.get_user(\"IUBloomington\")\n",
    "print(len(user.followers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, there are not only 15 Tweets using the hashtag #StarTrek50 (one of the trending tags when I made this notebook). Obviously, IU Bloomington doesn't have only 20 followers.\n",
    "\n",
    "Think about how you would browse a list of search results or a list of followers of @IUBloomington manually, on the Twitter website. It doesn't show you all of the items at once. Instead, you see a *certain amount* of items, then when you scroll down you see the site dynamically load additional results. This is an understandable limitation built into the website: if this behavior wasn't implemented, every time someone looked at the list of Katy Perry's Twitter followers the site server would have to load up over 95 million names; quite a server load. \n",
    "\n",
    "This principle is in place when you are accessing Twitter's API as well. When you call for a list of results or users or statuses, it doesn't give you *all* the relevant items at once, because to do so would tax the server.\n",
    "\n",
    "Instead, you have to go through a list of results in an iterative fashion. This is akin to *website pagination*, like when you look at at list of Google Search results, scroll down, and then click to the \"next page of results\". So if you want to get the first 1000 statuses that have a trending hashtag, you'll have to iterate through several \"pages\" of results. \n",
    "\n",
    "Each time you access a new \"page\" in a list of results, this will decrease the limited number of requests that Twitter allows each API client every 15 minutes. This limitation is known as Twitter's *rate limit* and it's imposed on every client that uses its API. \n",
    "\n",
    "Normally, you'd have to laboriously keep track of how many requests you are making and how many requests you have left. Fortunately, Tweepy helps us get around this in two ways. Remember that when we first initialized our ``api`` object, we set the parameters ``wait_on_rate_limit`` and ``wait_on_rate_limit_notify`` to ``True``. This means that if we do something taxing using this particular ``api`` object, it's smart enough to keep track of how many requests you have left, and to wait the proper amount of time before continuing to iterate through the results. \n",
    "\n",
    "The second thing we have to do is rely on a special Tweepy method called ``Cursor``. It's a bit more difficulty to use the cursor instead of the usual methods we learned above, but ultimately it'll serve you better if you're trying to get a large amount of Social Media data. Let's see how it works. \n",
    "\n",
    "First, we initialize a ``tweepy.Cursor`` object. But! ``Cursor`` is a method that takes an argument. That argument is the **method you want to iterate over**. \n",
    "\n",
    "```Python\n",
    "c = tweepy.Cursor(api.search)\n",
    "```\n",
    "\n",
    "Now, you might notice something unusual here. ``api.search`` is the argument we're passing to ``Cursor``, but it is, itself, a method that requires its own arguments. ``api.search`` needs to know what you're looking for: the ``q`` parameter. However, note that the arguments to the methods being passed should be passed as *additional arguments to the Cursor object*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = tweepy.Cursor(api.search, q=\"#Rio\", lang=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look carefully at the above code. Rather than passing the relevant arguments to ``api.search``, we're passing them as additional arguments to the Cursor object. \n",
    "\n",
    "Once we have the Cursor object, we call its ``items()`` method. The ``items()`` method takes the number of results you ultimately want to get. \n",
    "\n",
    "Usually, since you're handling a lot of results, you don't want to store the result of ``c.items(500)`` in a new variable. Instead, you want to immediately iterate through it using a ``for`` loop. This means that Python won't have to summon from Twitter all 500 (1000, 10000, etc) results in its environment at the same time. If you use ``c`` in a for loop, it can pull items from Twitter one at a time. \n",
    "\n",
    "You'll probably want to initialize some empty list to store the results of the iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_store = []\n",
    "\n",
    "for status in c.items(500):\n",
    "    tweet_store.append(status.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Via this method, we've successfully obtained the texts of 500 Tweets that are in English and contain the hashtag #rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "['#RockInRio #Rock #Rio #Skank #Rock SHOW https://t.co/vSC1fywmN6 via @YouTube', '#Beautiful #Rio Cum Girls #Hereford https://t.co/pfYTFGX0ZY']\n"
     ]
    }
   ],
   "source": [
    "print(len(tweet_store))\n",
    "print(tweet_store[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you could put numbers greater than 500 here, but that it will quite possibly take a long time. This will not be due to technical reasons; it will mostly be the result of Tweepy pausing when you are hitting your rate limit and waiting as long as necessary before you're in the green again. \n",
    "\n",
    "There are two options for handling this time problem. Either, early on, you download as much of the relevant data you have as possible, then save the data and work exclusively with that data moving forward. The advantage of this approach is that you have all the data you need for training the model, the disadvantage is that once you've gotten the data you're \"stuck\" with it. The other approach is to download a small, tractable set of data at first and then build your code for processing and analyzing the data. Once everything is in place, then you download a large set of data and then run everything on it. The advantage is that you can dedicate your time to developing your model and you can have the \"newest data possible\", the disadvantage is that you may run into unanticipated problems with your code once you get the final dataset - sure, it may work smoothly on your earlier, smaller dataset but then something unexpected may come up once you're working with the bigger data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Getting a User's Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, we've only been getting tweets as response to a particular query - a search term or a hashtag. The ``api`` we have created, however, also has a method called ``user_timeline`` that allows us to collect tweets from a single user. Using what we've just learned about the cursor objects, we can therefore use ``user_timeline`` to get a specified amount of the user's most recent tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = tweepy.Cursor(api.user_timeline, id=\"IndianaMBB\") # Instead of api.search, pass api.user_timeline\n",
    "\n",
    "tweet_store = []\n",
    "for status in c.items(500):\n",
    "    tweet_store.append(status.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(tweet_store))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"NOTES: #IUBB is 5-0 all-time against Rutgers heading into Monday's game in New Jersey (7pm, BTN) ‚ö™Ô∏èüî¥\\n\\nNotes:‚Ä¶ https://t.co/hSWLfHIuix\", 'On to Rutgers. https://t.co/sUnLNlA7C3']\n"
     ]
    }
   ],
   "source": [
    "print(tweet_store[:2])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
